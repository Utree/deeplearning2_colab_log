{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "5章.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Utree/deeplearning2_colab_log/blob/master/5%E7%AB%A0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "KiCIhc65AtD-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# common/config.py\n",
        "GPU = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qYUgw9Bl7JAO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# common/np.py\n",
        "if GPU:\n",
        "  import cupy as np\n",
        "  np.cuda.set_allocator(np.cuda.MemoryPool().malloc)\n",
        "  np.add.at = np.scatter_add\n",
        "  \n",
        "  print('\\033[92m' + '-' * 60 + '\\033[0m')\n",
        "  print(' ' * 23 + '\\033[92mGPU Mode (cupy)\\033[0m')\n",
        "  print('\\033[92m' + '-' * 60 + '\\033[0m\\n')\n",
        "else:\n",
        "  import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rJ0nRBIp9Trd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# common/functions.py\n",
        "def sigmoid(x):\n",
        "  return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def relu(x):\n",
        "  return np.maximum(0, x)\n",
        "\n",
        "def softmax(x):\n",
        "  if x.ndim == 2:\n",
        "    x = x - x.max(axis=1, keepdims=True)\n",
        "    x = np.exp(x)\n",
        "    x /= x.sum(axis=1, keepdims=True)\n",
        "  elif x.ndim == 1:\n",
        "    x = x - np.max(x)\n",
        "    x = np.exp(x) / np.sum(np.exp(x))\n",
        "    \n",
        "  return x\n",
        "\n",
        "def cross_entropy_error(y, t):\n",
        "  if y.ndim == 1:\n",
        "    t = t.reshape(1, t.size)\n",
        "    y = y.reshape(1, y.size)\n",
        "    \n",
        "  # 教師データがone-hot-vectorの場合、正解ラベルのインデックスに変換\n",
        "  if t.size == y.size:\n",
        "    t = t.argmax(axis=1)\n",
        "    \n",
        "  batch_size = y.shape[0]\n",
        "  \n",
        "  return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7_X8xgcX9awA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# common/layers.py\n",
        "class MatMul:\n",
        "  def __init__(self, W):\n",
        "    self.params = [W]\n",
        "    self.grads = [np.zeros_like(W)]\n",
        "    self.x = None\n",
        "    \n",
        "  def forward(self, x):\n",
        "    W, = self.params\n",
        "    out = np.dot(x, W)\n",
        "    self.x = x\n",
        "    return out\n",
        "  \n",
        "  def backward(self, dout):\n",
        "    W, = self.params\n",
        "    dx = np.dot(dout, W.T)\n",
        "    dW = np.dot(self.x.T, dout)\n",
        "    self.grads[0][...] = dW\n",
        "    return dx\n",
        "  \n",
        "class Affine:\n",
        "  def __init__(self, W, b):\n",
        "    self.params = [W, b]\n",
        "    self.grads = [np.zeros_like(W), np.zeros_like(b)]\n",
        "    self.x = None\n",
        "    \n",
        "  def forward(self, x):\n",
        "    W, b = self.params\n",
        "    out = np.dot(x, W) + b\n",
        "    self.x = x\n",
        "    return out\n",
        "  \n",
        "  def backward(self, dout):\n",
        "    W, b = self.params\n",
        "    dx = np.dot(dout, W.T)\n",
        "    dW = np.dot(self.x.T, dout)\n",
        "    db = np.sum(dout, axis=0)\n",
        "    \n",
        "    self.grads[0][...] = dW\n",
        "    self.grads[1][...] = db\n",
        "    return dx\n",
        "  \n",
        "class Softmax:\n",
        "  def __init__(self):\n",
        "    self.params, self.grads = [], []\n",
        "    self.out = None\n",
        "    \n",
        "  def forward(self, x):\n",
        "    self.out = softmax(x)\n",
        "    return self.out\n",
        "  \n",
        "  def backward(self, dout):\n",
        "    dx = self.out * dout\n",
        "    sumdx = np.sum(dx, axis=1, keepdims=True)\n",
        "    dx -= self.out * sumdx\n",
        "    return dx\n",
        "  \n",
        "class SoftmaxWithLoss:\n",
        "  def __init__(self):\n",
        "    self.params, self.grads = [], []\n",
        "    self.y = None # softmaxの出力\n",
        "    self.t = None # 教師ラベル\n",
        "    \n",
        "  def forward(self, x, t):\n",
        "    self.t = t\n",
        "    self.y = softmax(x)\n",
        "    \n",
        "    # 教師ラベルがone-hotベクトルの場合、正解のインデックスに変換\n",
        "    if self.t.size == self.y.size:\n",
        "      self.t = self.t.argmax(axis=1)\n",
        "      \n",
        "    loss = cross_entropy_error(self.y, self.t)\n",
        "    return loss\n",
        "  \n",
        "  def backward(self, dout=1):\n",
        "    batch_size = self.t.shape[0]\n",
        "    \n",
        "    dx = self.y.copy()\n",
        "    dx[np.arange(batch_size), self.t] -= 1\n",
        "    dx *= dout\n",
        "    dx = dx / batch_size\n",
        "    \n",
        "    return dx\n",
        "  \n",
        "class Sigmoid:\n",
        "  def __init__(self):\n",
        "    self.params, self.grads = [], []\n",
        "    self.out = None\n",
        "    \n",
        "  def forward(self, x):\n",
        "    out = 1 / (1 + np.exp(-x))\n",
        "    self.out = out\n",
        "    return out\n",
        "  \n",
        "  def backward(self, dout):\n",
        "    dx = dout * (1.0 - self.out) * self.out\n",
        "    return dx\n",
        "  \n",
        "class SigmoidWithLoss:\n",
        "  def __init__(self):\n",
        "    self.params, self.grads = [], []\n",
        "    self.loss = None\n",
        "    self.y = None # sigmoidの出力\n",
        "    self.t = None # 教師データ\n",
        "    \n",
        "  def forward(self, x, t):\n",
        "    self.t = t\n",
        "    self.y = 1 / (1 + np.exp(-x))\n",
        "    \n",
        "    self.loss = cross_entropy_error(np.c_[1 - self.y, self.y], self.t)\n",
        "    \n",
        "    return self.loss\n",
        "  \n",
        "  def backward(self, dout=1):\n",
        "    batch_size = self.t.shape[0]\n",
        "    \n",
        "    dx = (self.y - self.t) * dout / batch_size\n",
        "    return dx\n",
        "  \n",
        "class Dropout:\n",
        "  def __init__(self, dropout_ratio=0.5):\n",
        "    self.params, self.grads = [], []\n",
        "    self.dropout_ratio = dropout_ratio\n",
        "    self.mask = None\n",
        "    \n",
        "  def forward(self, x, train_flg=True):\n",
        "    if train_flg:\n",
        "      self.mask = np.random.rand(*x.shape) > self.dropout_ratio\n",
        "      return x * self.mask\n",
        "    else:\n",
        "      return x * (1.0 - self.dropout_ratio)\n",
        "    \n",
        "  def backward(self, dout):\n",
        "    return dout * self.mask\n",
        "  \n",
        "class Embedding:\n",
        "  def __init__(self, W):\n",
        "    self.params = [W]\n",
        "    self.grads = [np.zeros_like(W)]\n",
        "    self.idx = None\n",
        "    \n",
        "  def forward(self, idx):\n",
        "    W, = self.params\n",
        "    self.idx = idx\n",
        "    out = W[idx]\n",
        "    return out\n",
        "  \n",
        "  def backward(self, dout):\n",
        "    dW, = self.grads\n",
        "    dW[...] = 0\n",
        "    np.add.at(dW, self.idx, dout)\n",
        "    return None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gDS-3KdB9Kxt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# common/time_layers.py\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OZWTteavJFU1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "- 前章までのニューラルネットワークは、フィードフォワードと呼ばれるタイプのネットワーク\n",
        "- フィードフォワードとは、流れが1方向のネットワークを指す\n",
        "- フィードフォワード・ネットワークでは、時系列データを上手く扱えない\n",
        "- 時系列データの性質(パターン)を学習するためにRNN(リカレントニューラルネットワーク)を用いる"
      ]
    },
    {
      "metadata": {
        "id": "BMvIM4-dJwTP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "word2vecを確率の視点から見る"
      ]
    },
    {
      "metadata": {
        "id": "yhcJS93OKNCn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "word2vecのCBOWモデルでは、コンテキストの単語からターゲットとなる単語を推測する\n",
        "\n",
        "Wt-1 -> Wt <- Wt+1\n",
        "\n",
        "つまり、コンテキスト: Wt-1とWt+1が与えられたとき、ターゲットがWtとなる確率を求める\n",
        "\n",
        "これが、ウィンドウサイズ1のCBOWモデル"
      ]
    },
    {
      "metadata": {
        "id": "KxNojvSKJ3pb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "言語モデルは単語の並びに対して確率を与える\n",
        "\n",
        "与えられた単語の並びがどれだけ自然な単語の並びなのかを確率で評価する"
      ]
    },
    {
      "metadata": {
        "id": "lM3x3vkRMNUt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "言語モデルは様々なアプリケーションで利用できる\n",
        "\n",
        "- 人の発話からいくつかの文章を候補として生成し、文章として自然であるかをランク付けする\n",
        "- 新しい文章を生成する用途にも利用でき、単語列の自然さから確率分布に従って、単語をサンプリング(紡ぎ出す)し、文章を生成する\n",
        "　"
      ]
    },
    {
      "metadata": {
        "id": "h5XNORHxXe_X",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "数式を用いて言語モデルを記述すると、\n",
        "m個の単語からなる文章は、順序を考慮した上で単語が出現する確率とみなすことができ、\n",
        "同時確率 : P(W1, W2, ... Wm)と表す。"
      ]
    },
    {
      "metadata": {
        "id": "ocfA3_rFYqL8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "言語モデルが扱う事後確率は、t番目の単語をターゲットとして、t番目より左側の単語すべてをコンテキスト(条件)とする"
      ]
    },
    {
      "metadata": {
        "id": "voyodky_Y83-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "では、前章で登場した、word2vecのCBOWモデルを言語モデルに適応するためには、コンテキストのサイズをある値に限定することで、近似的に表すことができる"
      ]
    },
    {
      "metadata": {
        "id": "ppy_WtnmZnIT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "しかし、CBOWモデルには２つの問題点がある。\n",
        "\n",
        "- １つ目は、コンテキストの大きさを固定することで、それ以前に登場する単語を考慮することが出来ない\n",
        "- 2つ目は、CBOWモデルはコンテキスト内の単語の並びが無視されてしまう"
      ]
    },
    {
      "metadata": {
        "id": "bWSJvQz_Z9aH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "中間層の単語ベクトルを並びを考慮し、連結するアプローチもあるが、コンテキストのサイズに比例して、重みパラメータも増加してしまう\n",
        "\n",
        "そのため、RNNを用いる"
      ]
    },
    {
      "metadata": {
        "id": "NVfpFyRHaebu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "RNNとは、直訳で、循環するニューラルネットワークを意味する"
      ]
    },
    {
      "metadata": {
        "id": "GPtjvlb-arnY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "RNNレイヤでは、閉じた経路、ループする経路を持つ。\n",
        "\n",
        "ループすることで、過去のデータを記憶しながら、最新のデータへと更新することができる。"
      ]
    },
    {
      "metadata": {
        "id": "sICBJuqWj7PC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "RNNレイヤでの計算は次式で表される\n",
        "\n",
        "ht = tanh(ht-1Wh + xtWx + b)\n",
        "\n",
        "ここのtanhは活性化関数(シグモイド関数etc..)を表しており、数学的には、双曲線接線関数という。\n",
        "\n",
        "双曲線接線関数はシグモイド関数の線形変換(y=ax+b: 足したり、掛けたり、足したり、割ったり、幾何的には、ベクトル空間を実数倍、回転、折返ししているのと同値)\n",
        "\n",
        "シグモイド関数では、マイナスの値を取らなかったが、双曲線接線関数では、マイナスをとる"
      ]
    },
    {
      "metadata": {
        "id": "Ovm1FpvrbQoC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "ループを展開したあとのRNNは通常の誤差逆伝播法を使い、勾配を求める。\n",
        "\n",
        "このとき、時間方向に展開したニューラルネットワークの誤差逆伝播法という意味で、BPTT(Backpropagation Through Time)と呼ぶ。"
      ]
    },
    {
      "metadata": {
        "id": "IX1iB2pcdSzN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "長い時系列データを学習するとき、問題が起こる。\n",
        "\n",
        "長い時系列データでは、BPTTの計算リソースも大きくなるため、勾配が不安定(レイヤが長くなると、勾配が小さくなることがあり、前時刻へと届かなくなる)になり、メモリ使用量も増加する。"
      ]
    },
    {
      "metadata": {
        "id": "WCMz4YGFdoP_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "この解決策として、逆伝播のネットワークのつながりを断ち切る Truncated BPTTという手法が用いられる。\n",
        "(順伝播では、途切れることはない)"
      ]
    },
    {
      "metadata": {
        "id": "lJvUZYDXeB_t",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "RNNでは、データをシーケンシャル(順番)に与える必要がある"
      ]
    },
    {
      "metadata": {
        "id": "kIwzCSChe1ZS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "シーケンシャルにデータを与えるとき、バッチ処理では、開始位置を各バッチでずらす必要がある。\n",
        "\n",
        "また、データが途中で終端に達した場合は先頭に戻す対応が必要"
      ]
    },
    {
      "metadata": {
        "id": "jg4GAYqVhMtm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "これまで循環するニューラルネットワークであるRNNレイヤ、時系列データをまとめて処理するTime RNNレイヤを実装してきました。\n",
        "\n",
        "次は、時系列データを扱い、最後に損失を評価するRNNLMを実装する"
      ]
    },
    {
      "metadata": {
        "id": "yPyGvF1DyH8E",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "RNNLMで最終的な損失を求めるとき、平均値を出す。"
      ]
    },
    {
      "metadata": {
        "id": "l1pXX2rtyeP5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "RNNLMの重みとバイアスを初期化する際、前奏からのノード数を用いて、1/ √nの標準偏差を持つ文武で初期化する"
      ]
    },
    {
      "metadata": {
        "id": "YyWwNBfDy8KN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 言語モデルの評価\n",
        "\n",
        "言語モデルの予測性能の良さを評価する指標として、**パープレキシティ** がよく用いられる\n",
        "\n",
        "## パープレキシティとは\n",
        "確率の逆数\n",
        "\n",
        "直感的には、分岐数と解釈でき、次に取りうる選択肢の数とみなせる"
      ]
    }
  ]
}