{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3章.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Utree/deeplearning2_colab_log/blob/master/3%E7%AB%A0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "dRtOsVYwue4B",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 単語の分散表現 〜推論ベースの手法word2vec〜\n",
        "カウントベースの手法では、周囲の単語の頻度によって、共起行列を作り、その行列に対して、SVDを適応することで、密なベクトル単語の分散表現を獲得しました。\n",
        "\n",
        "しかし、カウントベースの手法には問題点があり、大規模なコーパスを扱う場合、巨大な行列を作ることになります。しかし、巨大な行列に対して、SVDを行うことは現実的ではありません。"
      ]
    },
    {
      "metadata": {
        "id": "MPWf3XOZvuMq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 推論ベースの手法\n",
        "\n",
        "コンテキストが与えられたときに、どのような単語が出現するのかを推論する手法\n",
        "\n",
        "推論問題を繰り返し解き、出現パーターンを学習する\n",
        "\n",
        "モデルにコンテキストを入力として与えると、モデルは各単語の出現確率を出力する"
      ]
    },
    {
      "metadata": {
        "id": "NiQyWWY1SXQ9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "one-hot表現で単語をベクトル表現で表すと\n",
        "\n",
        "ニューラルネットワークを構成する様々な「レイヤ」によって処理することができる"
      ]
    },
    {
      "metadata": {
        "id": "mpmxk9oIttvM",
        "colab_type": "code",
        "outputId": "cc442941-622d-4161-f0aa-c0aa17b508ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "c = np.array([[1, 0, 0, 0, 0, 0, 0]]) # 入力\n",
        "W = np.random.randn(7, 3) # 重み\n",
        "# バイアスを用いいない全結合層のニューラルネット = 「行列の積」\n",
        "h = np.dot(c, W) # 中間ノード\n",
        "print(h)\n",
        "\n",
        "# one-hotベクトルの積では、該当する重みの行ベクトルを抜き出すことに相当する"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 0.27538298 -1.58193412 -3.17133897]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "p5u18dAgZ_Na",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SLNz2oxLY4aF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class MatMul:\n",
        "  '''\n",
        "    MatMul(行列の積)レイヤ\n",
        "  '''\n",
        "  def __init__(self, W):\n",
        "    self.params = [W]\n",
        "    self.grads = [np.zeros_like(W)]\n",
        "    self.x = None\n",
        "    \n",
        "  def forward(self, x):\n",
        "    W, = self.params\n",
        "    out = np.dot(x, W)\n",
        "    self.x = x\n",
        "    return out\n",
        "  \n",
        "  def backward(self, dout):\n",
        "    W, = self.params\n",
        "    dx = np.dot(dout, W.T)\n",
        "    dW = np.dot(self.x.T, dout)\n",
        "    self.grads[0][...] = dW\n",
        "    return dx"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XyHZyMLUaCvI",
        "colab_type": "code",
        "outputId": "d3e44239-f7ec-4c36-ba16-aa96f543eeca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "# MatMulレイヤで実装\n",
        "c = np.array([[1, 0, 0, 0, 0, 0, 0]])\n",
        "W = np.random.randn(7, 3)\n",
        "layer = MatMul(W)\n",
        "h = layer.forward(c)\n",
        "print(h)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[-0.71588144  0.46093901 -1.41097394]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "1inrCi6bjfri",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "word2vecでは、ニューラルネットワークのモデルにcontinuous bag-of-words(CBOW)を用いる"
      ]
    },
    {
      "metadata": {
        "id": "jtbwT5-6yGCF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## CBOWのデータ処理構造\n",
        "\n",
        "入力層でone-hot表現のベクトル受け取り、中間層で全結合による変換後の値の平均値を出し、出力層で、各単語のスコアを算出する\n",
        "\n",
        "そのスコアにSoftmax関数を適応すると、確率が得られる\n",
        "\n",
        "そして、この確率には、単語の意味もエンコードされている"
      ]
    },
    {
      "metadata": {
        "id": "hVV8Fo7y5Wbe",
        "colab_type": "code",
        "outputId": "d66065f2-ef60-4866-a7cb-aa84ffd50321",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "cell_type": "code",
      "source": [
        "# サンプルのコンテキストデータ\n",
        "c0 = np.array([[1, 0, 0, 0, 0, 0, 0]])\n",
        "c1 = np.array([[0, 0, 1, 0, 0, 0, 0]])\n",
        "\n",
        "# 重みの初期化\n",
        "W_in = np.random.randn(7, 3)\n",
        "W_out = np.random.randn(3, 7)\n",
        "\n",
        "# レイヤの生成\n",
        "in_layer0 = MatMul(W_in)\n",
        "in_layer1 = MatMul(W_in)\n",
        "out_layer = MatMul(W_out)\n",
        "\n",
        "# 順伝播\n",
        "h0 = in_layer0.forward(c0)\n",
        "h1 = in_layer1.forward(c1)\n",
        "h = 0.5 * (h0 + h1)\n",
        "s = out_layer.forward(h)\n",
        "\n",
        "print(s)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[-0.74446532 -0.66704092  1.06909781  0.2488699   1.38501813  0.7607661\n",
            "   0.77385235]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "xaPt9g48a-rM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def preprocess(text):\n",
        "  text = text.lower()\n",
        "  text = text.replace('.', ' .')\n",
        "  words = text.split(' ')\n",
        "  \n",
        "  word_to_id = {}\n",
        "  id_to_word = {}\n",
        "  for word in words:\n",
        "    if word not in word_to_id:\n",
        "      new_id = len(word_to_id)\n",
        "      word_to_id[word] = new_id\n",
        "      id_to_word[new_id] = word\n",
        "  \n",
        "  corpus = np.array([word_to_id[w] for w in words])\n",
        "  \n",
        "  return corpus, word_to_id, id_to_word"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ohpK_SFXcLLh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "366f328f-3282-4732-b927-34fc4b36b1c7"
      },
      "cell_type": "code",
      "source": [
        "text = 'You say goodbye and I say hello.'\n",
        "corpus, word_to_id, id_to_word = preprocess(text)\n",
        "print(corpus)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0 1 2 3 4 1 5 6]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "WIZCfzsqcZvG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "8c579772-1e19-43de-8d4a-c833dd6aaccd"
      },
      "cell_type": "code",
      "source": [
        "print(id_to_word)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{0: 'you', 1: 'say', 2: 'goodbye', 3: 'and', 4: 'i', 5: 'hello', 6: '.'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "xkCf_xHocmJp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def create_contexts_target(corpus, window_size=1):\n",
        "  target = corpus[window_size:-window_size]\n",
        "  contexts = []\n",
        "  \n",
        "  for idx in range(window_size, len(corpus)-window_size):\n",
        "    cs = []\n",
        "    for t in range(-window_size, window_size + 1):\n",
        "      if t == 0:\n",
        "        continue\n",
        "      cs.append(corpus[idx + t])\n",
        "    contexts.append(cs)\n",
        "  \n",
        "  return np.array(contexts), np.array(target)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "T4DOLVi0dW1q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "contexts, target = create_contexts_target(corpus, window_size=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "C1UeAnzhddDD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "outputId": "182f91c1-3036-42c9-c26e-bb7c07975c69"
      },
      "cell_type": "code",
      "source": [
        "print(contexts)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0 2]\n",
            " [1 3]\n",
            " [2 4]\n",
            " [3 1]\n",
            " [4 5]\n",
            " [1 6]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "A1xweVBkdgXW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "751ac5b3-834a-4423-b93b-cb3e1eac0f20"
      },
      "cell_type": "code",
      "source": [
        "print(target)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 2 3 4 1 5]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "2GX3zjFdeKrT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def convert_one_hot(corpus, vocab_size):\n",
        "  '''one-hot表現への変換\n",
        "  \n",
        "  :param corpus: 単語IDのリスト(1次元もしくは2次元のNumPy配列)\n",
        "  :param vocab_size: 語彙数\n",
        "  :return: one-hot表現(2次元もしくは3次元のNumpy配列)\n",
        "  '''\n",
        "  N = corpus.shape[0]\n",
        "  \n",
        "  if corpus.ndim == 1:\n",
        "    one_hot = np.zeros((N, vocab_size), dtype=np.int32)\n",
        "    for idx, word_id in enumerate(corpus):\n",
        "      one_hot[idx, word_id] = 1\n",
        "  \n",
        "  elif corpus.ndim == 2:\n",
        "    C = corpus.shape[1]\n",
        "    one_hot = np.zeros((N, C, vocab_size), dtype=np.int32)\n",
        "    for idx_0, word_ids in enumerate(corpus):\n",
        "      for idx_1, word_id in enumerate(word_ids):\n",
        "        one_hot[idx_0, idx_1, word_id] = 1\n",
        "  \n",
        "  return one_hot"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8sJXEZzEfmCn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "text = 'You say goodbye and I say hello.'\n",
        "corpus, word_to_id, id_to_word = preprocess(text)\n",
        "\n",
        "contexts, target = create_contexts_target(corpus, window_size=1)\n",
        "\n",
        "vocab_size = len(word_to_id)\n",
        "target = convert_one_hot(target, vocab_size)\n",
        "contexts = convert_one_hot(contexts, vocab_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wyiSOBo6gihT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def softmax(x):\n",
        "  if x.ndim == 2:\n",
        "    x = x - x.max(axis=1, keepdims=True)\n",
        "    x = np.exp(x)\n",
        "    x /= x.sum(axis=1, keepdims=True)\n",
        "  elif x.ndim == 1:\n",
        "    x = x - np.max(x)\n",
        "    x = np.exp(x) / np.sum(np.exp(x))\n",
        "    \n",
        "  return x\n",
        "\n",
        "def cross_entropy_error(y, t):\n",
        "  if y.ndim == 1:\n",
        "    t = t.reshape(1, t.size)\n",
        "    y = y.reshape(1, y.size)\n",
        "    \n",
        "  # 教師データがone-hot-vectorの場合、正解ラベルのインデックスに変換\n",
        "  if t.size == y.size:\n",
        "    t = t.argmax(axis=1)\n",
        "    \n",
        "  batch_size = y.shape[0]\n",
        "  \n",
        "  return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "w0dVmysdhnTo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class MatMul:\n",
        "  def __init__(self, W):\n",
        "    self.params = [W]\n",
        "    self.grads = [np.zeros_like(W)]\n",
        "    self.x = None\n",
        "    \n",
        "  def forward(self, x):\n",
        "    W, = self.params\n",
        "    out = np.dot(x, W)\n",
        "    self.x = x\n",
        "    return out\n",
        "  \n",
        "  def backward(self, dout):\n",
        "    W, = self.params\n",
        "    dx = np.dot(dout, W.T)\n",
        "    dW = np.dot(self.x.T, dout)\n",
        "    self.grads[0][...] = dW\n",
        "    return dx"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YRgQH0HhibJx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class SoftmaxWithLoss:\n",
        "  def __init__(self):\n",
        "    self.params, self.grads = [], []\n",
        "    self.y = None\n",
        "    self.t = None\n",
        "    \n",
        "  def forward(self, x, t):\n",
        "    self.t = t\n",
        "    self.y = softmax(x)\n",
        "    \n",
        "    # 教師ラベルがone-hotベクトルの場合、正解のインデックスに変換\n",
        "    if self.t.size == self.y.size:\n",
        "      self.t = self.t.argmax(axis=1)\n",
        "      \n",
        "    loss = cross_entropy_error(self.y, self.t)\n",
        "    return loss\n",
        "  \n",
        "  def backward(self, dout=1):\n",
        "    batch_size = self.t.shape[0]\n",
        "    \n",
        "    dx = self.y.copy()\n",
        "    dx[np.arange(batch_size), self.t] -= 1\n",
        "    dx *= dout\n",
        "    dx = dx / batch_size\n",
        "    \n",
        "    return dx"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_niknFz8jOHO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class SimpleCBOW:\n",
        "  def __init__(self, vocab_size, hidden_size):\n",
        "    V, H = vocab_size, hidden_size\n",
        "    \n",
        "    # 重みの初期化\n",
        "    W_in = 0.01 * np.random.randn(V, H).astype('f')\n",
        "    W_out = 0.01 * np.random.randn(H, V).astype('f')\n",
        "    \n",
        "    # レイヤの生成\n",
        "    self.in_layer0 = MatMul(W_in)\n",
        "    self.in_layer1 = MatMul(W_in)\n",
        "    self.out_layer = MatMul(W_out)\n",
        "    self.loss_layer = SoftmaxWithLoss()\n",
        "    \n",
        "    # すべての重みと勾配をリストにまとめる\n",
        "    layers = [self.in_layer0, self.in_layer1, self.out_layer]\n",
        "    self.params, self.grads = [], []\n",
        "    for layer in layers:\n",
        "      self.params += layer.params\n",
        "      self.grads += layer.grads\n",
        "      \n",
        "    # メンバ変数に単語の分散表現を設定\n",
        "    self.word_vecs = W_in\n",
        "    \n",
        "  def forward(self, contexts, target):\n",
        "    h0 = self.in_layer0.forward(contexts[:, 0])\n",
        "    h1 = self.in_layer1.forward(contexts[:, 1])\n",
        "    h = (h0 + h1) * 0.5\n",
        "    score = self.out_layer.forward(h)\n",
        "    loss = self.loss_layer.forward(score, target)\n",
        "    return loss\n",
        "  \n",
        "  def backward(self, dout=1):\n",
        "    ds = self.loss_layer.backward(dout)\n",
        "    da = self.out_layer.backward(ds)\n",
        "    da *= 0.5\n",
        "    self.in_layer1.backward(da)\n",
        "    self.in_layer0.backward(da)\n",
        "    return None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vtdJHoRAl4Pu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy\n",
        "\n",
        "def clip_grads(grads, max_norm):\n",
        "  total_norm = 0\n",
        "  for grad in grads:\n",
        "    total_norm += np.sum(grad ** 2)\n",
        "  total_norm = np.sqrt(total_norm)\n",
        "  \n",
        "  rate = max_norm / (total_norm + 1e-6)\n",
        "  if rate < 1:\n",
        "    for grad in grads:\n",
        "      grad *= rate"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HxwSpk2TtzeO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def remove_duplicate(params, grads):\n",
        "    '''\n",
        "    パラメータ配列中の重複する重みをひとつに集約し、\n",
        "    その重みに対応する勾配を加算する\n",
        "    '''\n",
        "    params, grads = params[:], grads[:]  # copy list\n",
        "\n",
        "    while True:\n",
        "        find_flg = False\n",
        "        L = len(params)\n",
        "\n",
        "        for i in range(0, L - 1):\n",
        "            for j in range(i + 1, L):\n",
        "                # 重みを共有する場合\n",
        "                if params[i] is params[j]:\n",
        "                    grads[i] += grads[j]  # 勾配の加算\n",
        "                    find_flg = True\n",
        "                    params.pop(j)\n",
        "                    grads.pop(j)\n",
        "                # 転置行列として重みを共有する場合（weight tying）\n",
        "                elif params[i].ndim == 2 and params[j].ndim == 2 and \\\n",
        "                     params[i].T.shape == params[j].shape and np.all(params[i].T == params[j]):\n",
        "                    grads[i] += grads[j].T\n",
        "                    find_flg = True\n",
        "                    params.pop(j)\n",
        "                    grads.pop(j)\n",
        "\n",
        "                if find_flg: break\n",
        "            if find_flg: break\n",
        "\n",
        "        if not find_flg: break\n",
        "\n",
        "    return params, grads\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "w19xQGtfm3OY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Trainer:\n",
        "  def __init__(self, model, optimizer):\n",
        "    self.model = model\n",
        "    self.optimizer = optimizer\n",
        "    self.loss_list = []\n",
        "    self.eval_interval = None\n",
        "    self.current_epoch = 0\n",
        "    \n",
        "  def fit(self, x, t, max_epoch=10, batch_size=32, max_grad=None, eval_interval=20):\n",
        "    data_size = len(x)\n",
        "    max_iters = data_size // batch_size\n",
        "    self.eval_interval = eval_interval\n",
        "    model, optimizer = self.model, self.optimizer\n",
        "    total_loss = 0\n",
        "    loss_count = 0\n",
        "    \n",
        "    start_time = time.time()\n",
        "    for epoch in range(max_epoch):\n",
        "      # シャッフル\n",
        "      idx = numpy.random.permutation(numpy.arange(data_size))\n",
        "      x = x[idx]\n",
        "      t = t[idx]\n",
        "      \n",
        "      for iters in range(max_iters):\n",
        "        batch_x = x[iters*batch_size:(iters+1)*batch_size]\n",
        "        batch_t = t[iters*batch_size:(iters+1)*batch_size]\n",
        "        \n",
        "        # 勾配を求め、パラメータを更新\n",
        "        loss = model.forward(batch_x, batch_t)\n",
        "        model.backward()\n",
        "        params, grads = remove_duplicate(model.params, model.grads)\n",
        "        # 共有された重みを1つに集約\n",
        "        if max_grad is not None:\n",
        "          clip_grads(grads, max_grad)\n",
        "        optimizer.update(params, grads)\n",
        "        total_loss += loss\n",
        "        loss_count += 1\n",
        "        \n",
        "        # 評価\n",
        "        if(eval_interval is not None) and (iters % eval_interval) == 0:\n",
        "          avg_loss = total_loss / loss_count\n",
        "          elapsed_time = time.time() - start_time\n",
        "          print('| epoch %d | iter %d / %d | time %d[s] | loss %.2f'\n",
        "                %(self.current_epoch + 1, iters + 1, max_iters,\n",
        "                 elapsed_time, avg_loss))\n",
        "          self.loss_list.append(float(avg_loss))\n",
        "          total_loss, loss_count = 0, 0\n",
        "          \n",
        "      self.current_epoch +=1\n",
        "      \n",
        "  def plot(self, ylim=None):\n",
        "    x = numpy.arange(len(self.loss_list))\n",
        "    if ylim is not None:\n",
        "      plt.ylim(*ylim)\n",
        "    plt.plot(x, self.loss_list, label='train')\n",
        "    plt.xlabel('iterations (x' + str(self.eval_interval) + ')')\n",
        "    plt.ylabel('loss')\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ggj2pvpOq_du",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Adam:\n",
        "  '''\n",
        "  Adam (http://arxiv.org/abs/1412.6980v8)\n",
        "  '''\n",
        "  def __init__(self, lr=0.001, beta1=0.9, beta2=0.999):\n",
        "    self.lr = lr\n",
        "    self.beta1 = beta1\n",
        "    self.beta2 = beta2\n",
        "    self.iter = 0\n",
        "    self.m = None\n",
        "    self.v = None\n",
        "    \n",
        "  def update(self, params, grads):\n",
        "    if self.m is None:\n",
        "      self.m, self.v = [], []\n",
        "      for param in params:\n",
        "        self.m.append(np.zeros_like(param))\n",
        "        self.v.append(np.zeros_like(param))\n",
        "        \n",
        "    self.iter += 1\n",
        "    lr_t = self.lr * np.sqrt(1.0 - self.beta2**self.iter) / (1.0 - self.beta1**self.iter)\n",
        "    \n",
        "    for i in range(len(params)):\n",
        "      self.m[i] += (1 - self.beta1) * (grads[i] - self.m[i])\n",
        "      self.v[i] += (1 - self.beta2) * (grads[i]**2 - self.v[i])\n",
        "      \n",
        "      params[i] -= lr_t * self.m[i] / (np.sqrt(self.v[i]) + 1e-7)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oGozO1f1spZM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 18361
        },
        "outputId": "df3cf340-3453-4068-cdf1-ae76131fc59b"
      },
      "cell_type": "code",
      "source": [
        "window_size = 1\n",
        "hidden_size = 5\n",
        "batch_size = 3\n",
        "max_epoch = 1000\n",
        "\n",
        "text = 'You say goodbye and I say hello'\n",
        "corpus, word_to_id, id_to_word = preprocess(text)\n",
        "\n",
        "vocab_size = len(word_to_id)\n",
        "contexts, target = create_contexts_target(corpus, window_size)\n",
        "target = convert_one_hot(target, vocab_size)\n",
        "contexts = convert_one_hot(contexts, vocab_size)\n",
        "\n",
        "model = SimpleCBOW(vocab_size, hidden_size)\n",
        "optimizer = Adam()\n",
        "trainer = Trainer(model, optimizer)\n",
        "\n",
        "trainer.fit(contexts, target, max_epoch, batch_size)\n",
        "trainer.plot()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "| epoch 1 | iter 1 / 1 | time 0[s] | loss 1.79\n",
            "| epoch 2 | iter 1 / 1 | time 0[s] | loss 1.79\n",
            "| epoch 3 | iter 1 / 1 | time 0[s] | loss 1.79\n",
            "| epoch 4 | iter 1 / 1 | time 0[s] | loss 1.79\n",
            "| epoch 5 | iter 1 / 1 | time 0[s] | loss 1.79\n",
            "| epoch 6 | iter 1 / 1 | time 0[s] | loss 1.79\n",
            "| epoch 7 | iter 1 / 1 | time 0[s] | loss 1.79\n",
            "| epoch 8 | iter 1 / 1 | time 0[s] | loss 1.79\n",
            "| epoch 9 | iter 1 / 1 | time 0[s] | loss 1.79\n",
            "| epoch 10 | iter 1 / 1 | time 0[s] | loss 1.79\n",
            "| epoch 11 | iter 1 / 1 | time 0[s] | loss 1.79\n",
            "| epoch 12 | iter 1 / 1 | time 0[s] | loss 1.79\n",
            "| epoch 13 | iter 1 / 1 | time 0[s] | loss 1.79\n",
            "| epoch 14 | iter 1 / 1 | time 0[s] | loss 1.79\n",
            "| epoch 15 | iter 1 / 1 | time 0[s] | loss 1.79\n",
            "| epoch 16 | iter 1 / 1 | time 0[s] | loss 1.79\n",
            "| epoch 17 | iter 1 / 1 | time 0[s] | loss 1.79\n",
            "| epoch 18 | iter 1 / 1 | time 0[s] | loss 1.79\n",
            "| epoch 19 | iter 1 / 1 | time 0[s] | loss 1.79\n",
            "| epoch 20 | iter 1 / 1 | time 0[s] | loss 1.79\n",
            "| epoch 21 | iter 1 / 1 | time 0[s] | loss 1.79\n",
            "| epoch 22 | iter 1 / 1 | time 0[s] | loss 1.79\n",
            "| epoch 23 | iter 1 / 1 | time 0[s] | loss 1.79\n",
            "| epoch 24 | iter 1 / 1 | time 0[s] | loss 1.79\n",
            "| epoch 25 | iter 1 / 1 | time 0[s] | loss 1.79\n",
            "| epoch 26 | iter 1 / 1 | time 0[s] | loss 1.79\n",
            "| epoch 27 | iter 1 / 1 | time 0[s] | loss 1.79\n",
            "| epoch 28 | iter 1 / 1 | time 0[s] | loss 1.79\n",
            "| epoch 29 | iter 1 / 1 | time 0[s] | loss 1.79\n",
            "| epoch 30 | iter 1 / 1 | time 0[s] | loss 1.79\n",
            "| epoch 31 | iter 1 / 1 | time 0[s] | loss 1.79\n",
            "| epoch 32 | iter 1 / 1 | time 0[s] | loss 1.79\n",
            "| epoch 33 | iter 1 / 1 | time 0[s] | loss 1.79\n",
            "| epoch 34 | iter 1 / 1 | time 0[s] | loss 1.79\n",
            "| epoch 35 | iter 1 / 1 | time 0[s] | loss 1.79\n",
            "| epoch 36 | iter 1 / 1 | time 0[s] | loss 1.79\n",
            "| epoch 37 | iter 1 / 1 | time 0[s] | loss 1.79\n",
            "| epoch 38 | iter 1 / 1 | time 0[s] | loss 1.79\n",
            "| epoch 39 | iter 1 / 1 | time 0[s] | loss 1.79\n",
            "| epoch 40 | iter 1 / 1 | time 0[s] | loss 1.79\n",
            "| epoch 41 | iter 1 / 1 | time 0[s] | loss 1.79\n",
            "| epoch 42 | iter 1 / 1 | time 0[s] | loss 1.79\n",
            "| epoch 43 | iter 1 / 1 | time 0[s] | loss 1.79\n",
            "| epoch 44 | iter 1 / 1 | time 0[s] | loss 1.79\n",
            "| epoch 45 | iter 1 / 1 | time 0[s] | loss 1.79\n",
            "| epoch 46 | iter 1 / 1 | time 0[s] | loss 1.78\n",
            "| epoch 47 | iter 1 / 1 | time 0[s] | loss 1.78\n",
            "| epoch 48 | iter 1 / 1 | time 0[s] | loss 1.78\n",
            "| epoch 49 | iter 1 / 1 | time 0[s] | loss 1.78\n",
            "| epoch 50 | iter 1 / 1 | time 0[s] | loss 1.78\n",
            "| epoch 51 | iter 1 / 1 | time 0[s] | loss 1.78\n",
            "| epoch 52 | iter 1 / 1 | time 0[s] | loss 1.78\n",
            "| epoch 53 | iter 1 / 1 | time 0[s] | loss 1.78\n",
            "| epoch 54 | iter 1 / 1 | time 0[s] | loss 1.78\n",
            "| epoch 55 | iter 1 / 1 | time 0[s] | loss 1.78\n",
            "| epoch 56 | iter 1 / 1 | time 0[s] | loss 1.78\n",
            "| epoch 57 | iter 1 / 1 | time 0[s] | loss 1.78\n",
            "| epoch 58 | iter 1 / 1 | time 0[s] | loss 1.78\n",
            "| epoch 59 | iter 1 / 1 | time 0[s] | loss 1.78\n",
            "| epoch 60 | iter 1 / 1 | time 0[s] | loss 1.78\n",
            "| epoch 61 | iter 1 / 1 | time 0[s] | loss 1.78\n",
            "| epoch 62 | iter 1 / 1 | time 0[s] | loss 1.78\n",
            "| epoch 63 | iter 1 / 1 | time 0[s] | loss 1.78\n",
            "| epoch 64 | iter 1 / 1 | time 0[s] | loss 1.78\n",
            "| epoch 65 | iter 1 / 1 | time 0[s] | loss 1.77\n",
            "| epoch 66 | iter 1 / 1 | time 0[s] | loss 1.77\n",
            "| epoch 67 | iter 1 / 1 | time 0[s] | loss 1.77\n",
            "| epoch 68 | iter 1 / 1 | time 0[s] | loss 1.77\n",
            "| epoch 69 | iter 1 / 1 | time 0[s] | loss 1.77\n",
            "| epoch 70 | iter 1 / 1 | time 0[s] | loss 1.77\n",
            "| epoch 71 | iter 1 / 1 | time 0[s] | loss 1.77\n",
            "| epoch 72 | iter 1 / 1 | time 0[s] | loss 1.77\n",
            "| epoch 73 | iter 1 / 1 | time 0[s] | loss 1.77\n",
            "| epoch 74 | iter 1 / 1 | time 0[s] | loss 1.77\n",
            "| epoch 75 | iter 1 / 1 | time 0[s] | loss 1.77\n",
            "| epoch 76 | iter 1 / 1 | time 0[s] | loss 1.76\n",
            "| epoch 77 | iter 1 / 1 | time 0[s] | loss 1.77\n",
            "| epoch 78 | iter 1 / 1 | time 0[s] | loss 1.76\n",
            "| epoch 79 | iter 1 / 1 | time 0[s] | loss 1.76\n",
            "| epoch 80 | iter 1 / 1 | time 0[s] | loss 1.76\n",
            "| epoch 81 | iter 1 / 1 | time 0[s] | loss 1.76\n",
            "| epoch 82 | iter 1 / 1 | time 0[s] | loss 1.76\n",
            "| epoch 83 | iter 1 / 1 | time 0[s] | loss 1.76\n",
            "| epoch 84 | iter 1 / 1 | time 0[s] | loss 1.75\n",
            "| epoch 85 | iter 1 / 1 | time 0[s] | loss 1.76\n",
            "| epoch 86 | iter 1 / 1 | time 0[s] | loss 1.75\n",
            "| epoch 87 | iter 1 / 1 | time 0[s] | loss 1.75\n",
            "| epoch 88 | iter 1 / 1 | time 0[s] | loss 1.75\n",
            "| epoch 89 | iter 1 / 1 | time 0[s] | loss 1.75\n",
            "| epoch 90 | iter 1 / 1 | time 0[s] | loss 1.75\n",
            "| epoch 91 | iter 1 / 1 | time 0[s] | loss 1.75\n",
            "| epoch 92 | iter 1 / 1 | time 0[s] | loss 1.75\n",
            "| epoch 93 | iter 1 / 1 | time 0[s] | loss 1.75\n",
            "| epoch 94 | iter 1 / 1 | time 0[s] | loss 1.74\n",
            "| epoch 95 | iter 1 / 1 | time 0[s] | loss 1.74\n",
            "| epoch 96 | iter 1 / 1 | time 0[s] | loss 1.74\n",
            "| epoch 97 | iter 1 / 1 | time 0[s] | loss 1.74\n",
            "| epoch 98 | iter 1 / 1 | time 0[s] | loss 1.74\n",
            "| epoch 99 | iter 1 / 1 | time 0[s] | loss 1.74\n",
            "| epoch 100 | iter 1 / 1 | time 0[s] | loss 1.74\n",
            "| epoch 101 | iter 1 / 1 | time 0[s] | loss 1.73\n",
            "| epoch 102 | iter 1 / 1 | time 0[s] | loss 1.73\n",
            "| epoch 103 | iter 1 / 1 | time 0[s] | loss 1.73\n",
            "| epoch 104 | iter 1 / 1 | time 0[s] | loss 1.73\n",
            "| epoch 105 | iter 1 / 1 | time 0[s] | loss 1.73\n",
            "| epoch 106 | iter 1 / 1 | time 0[s] | loss 1.73\n",
            "| epoch 107 | iter 1 / 1 | time 0[s] | loss 1.72\n",
            "| epoch 108 | iter 1 / 1 | time 0[s] | loss 1.72\n",
            "| epoch 109 | iter 1 / 1 | time 0[s] | loss 1.72\n",
            "| epoch 110 | iter 1 / 1 | time 0[s] | loss 1.72\n",
            "| epoch 111 | iter 1 / 1 | time 0[s] | loss 1.72\n",
            "| epoch 112 | iter 1 / 1 | time 0[s] | loss 1.72\n",
            "| epoch 113 | iter 1 / 1 | time 0[s] | loss 1.71\n",
            "| epoch 114 | iter 1 / 1 | time 0[s] | loss 1.72\n",
            "| epoch 115 | iter 1 / 1 | time 0[s] | loss 1.71\n",
            "| epoch 116 | iter 1 / 1 | time 0[s] | loss 1.71\n",
            "| epoch 117 | iter 1 / 1 | time 0[s] | loss 1.71\n",
            "| epoch 118 | iter 1 / 1 | time 0[s] | loss 1.70\n",
            "| epoch 119 | iter 1 / 1 | time 0[s] | loss 1.71\n",
            "| epoch 120 | iter 1 / 1 | time 0[s] | loss 1.71\n",
            "| epoch 121 | iter 1 / 1 | time 0[s] | loss 1.70\n",
            "| epoch 122 | iter 1 / 1 | time 0[s] | loss 1.70\n",
            "| epoch 123 | iter 1 / 1 | time 0[s] | loss 1.70\n",
            "| epoch 124 | iter 1 / 1 | time 0[s] | loss 1.70\n",
            "| epoch 125 | iter 1 / 1 | time 0[s] | loss 1.69\n",
            "| epoch 126 | iter 1 / 1 | time 0[s] | loss 1.69\n",
            "| epoch 127 | iter 1 / 1 | time 0[s] | loss 1.69\n",
            "| epoch 128 | iter 1 / 1 | time 0[s] | loss 1.69\n",
            "| epoch 129 | iter 1 / 1 | time 0[s] | loss 1.69\n",
            "| epoch 130 | iter 1 / 1 | time 0[s] | loss 1.68\n",
            "| epoch 131 | iter 1 / 1 | time 0[s] | loss 1.68\n",
            "| epoch 132 | iter 1 / 1 | time 0[s] | loss 1.69\n",
            "| epoch 133 | iter 1 / 1 | time 0[s] | loss 1.68\n",
            "| epoch 134 | iter 1 / 1 | time 0[s] | loss 1.68\n",
            "| epoch 135 | iter 1 / 1 | time 0[s] | loss 1.68\n",
            "| epoch 136 | iter 1 / 1 | time 0[s] | loss 1.67\n",
            "| epoch 137 | iter 1 / 1 | time 0[s] | loss 1.67\n",
            "| epoch 138 | iter 1 / 1 | time 0[s] | loss 1.66\n",
            "| epoch 139 | iter 1 / 1 | time 0[s] | loss 1.67\n",
            "| epoch 140 | iter 1 / 1 | time 0[s] | loss 1.66\n",
            "| epoch 141 | iter 1 / 1 | time 0[s] | loss 1.67\n",
            "| epoch 142 | iter 1 / 1 | time 0[s] | loss 1.66\n",
            "| epoch 143 | iter 1 / 1 | time 0[s] | loss 1.66\n",
            "| epoch 144 | iter 1 / 1 | time 0[s] | loss 1.65\n",
            "| epoch 145 | iter 1 / 1 | time 0[s] | loss 1.65\n",
            "| epoch 146 | iter 1 / 1 | time 0[s] | loss 1.65\n",
            "| epoch 147 | iter 1 / 1 | time 0[s] | loss 1.64\n",
            "| epoch 148 | iter 1 / 1 | time 0[s] | loss 1.65\n",
            "| epoch 149 | iter 1 / 1 | time 0[s] | loss 1.65\n",
            "| epoch 150 | iter 1 / 1 | time 0[s] | loss 1.65\n",
            "| epoch 151 | iter 1 / 1 | time 0[s] | loss 1.65\n",
            "| epoch 152 | iter 1 / 1 | time 0[s] | loss 1.65\n",
            "| epoch 153 | iter 1 / 1 | time 0[s] | loss 1.65\n",
            "| epoch 154 | iter 1 / 1 | time 0[s] | loss 1.64\n",
            "| epoch 155 | iter 1 / 1 | time 0[s] | loss 1.62\n",
            "| epoch 156 | iter 1 / 1 | time 0[s] | loss 1.63\n",
            "| epoch 157 | iter 1 / 1 | time 0[s] | loss 1.62\n",
            "| epoch 158 | iter 1 / 1 | time 0[s] | loss 1.62\n",
            "| epoch 159 | iter 1 / 1 | time 0[s] | loss 1.62\n",
            "| epoch 160 | iter 1 / 1 | time 0[s] | loss 1.62\n",
            "| epoch 161 | iter 1 / 1 | time 0[s] | loss 1.62\n",
            "| epoch 162 | iter 1 / 1 | time 0[s] | loss 1.62\n",
            "| epoch 163 | iter 1 / 1 | time 0[s] | loss 1.63\n",
            "| epoch 164 | iter 1 / 1 | time 0[s] | loss 1.62\n",
            "| epoch 165 | iter 1 / 1 | time 0[s] | loss 1.61\n",
            "| epoch 166 | iter 1 / 1 | time 0[s] | loss 1.61\n",
            "| epoch 167 | iter 1 / 1 | time 0[s] | loss 1.61\n",
            "| epoch 168 | iter 1 / 1 | time 0[s] | loss 1.62\n",
            "| epoch 169 | iter 1 / 1 | time 0[s] | loss 1.60\n",
            "| epoch 170 | iter 1 / 1 | time 0[s] | loss 1.60\n",
            "| epoch 171 | iter 1 / 1 | time 0[s] | loss 1.60\n",
            "| epoch 172 | iter 1 / 1 | time 0[s] | loss 1.61\n",
            "| epoch 173 | iter 1 / 1 | time 0[s] | loss 1.59\n",
            "| epoch 174 | iter 1 / 1 | time 0[s] | loss 1.58\n",
            "| epoch 175 | iter 1 / 1 | time 0[s] | loss 1.58\n",
            "| epoch 176 | iter 1 / 1 | time 0[s] | loss 1.59\n",
            "| epoch 177 | iter 1 / 1 | time 0[s] | loss 1.57\n",
            "| epoch 178 | iter 1 / 1 | time 0[s] | loss 1.58\n",
            "| epoch 179 | iter 1 / 1 | time 0[s] | loss 1.57\n",
            "| epoch 180 | iter 1 / 1 | time 0[s] | loss 1.56\n",
            "| epoch 181 | iter 1 / 1 | time 0[s] | loss 1.57\n",
            "| epoch 182 | iter 1 / 1 | time 0[s] | loss 1.56\n",
            "| epoch 183 | iter 1 / 1 | time 0[s] | loss 1.56\n",
            "| epoch 184 | iter 1 / 1 | time 0[s] | loss 1.57\n",
            "| epoch 185 | iter 1 / 1 | time 0[s] | loss 1.55\n",
            "| epoch 186 | iter 1 / 1 | time 0[s] | loss 1.57\n",
            "| epoch 187 | iter 1 / 1 | time 0[s] | loss 1.55\n",
            "| epoch 188 | iter 1 / 1 | time 0[s] | loss 1.54\n",
            "| epoch 189 | iter 1 / 1 | time 0[s] | loss 1.55\n",
            "| epoch 190 | iter 1 / 1 | time 0[s] | loss 1.55\n",
            "| epoch 191 | iter 1 / 1 | time 0[s] | loss 1.54\n",
            "| epoch 192 | iter 1 / 1 | time 0[s] | loss 1.55\n",
            "| epoch 193 | iter 1 / 1 | time 0[s] | loss 1.54\n",
            "| epoch 194 | iter 1 / 1 | time 0[s] | loss 1.53\n",
            "| epoch 195 | iter 1 / 1 | time 0[s] | loss 1.54\n",
            "| epoch 196 | iter 1 / 1 | time 0[s] | loss 1.53\n",
            "| epoch 197 | iter 1 / 1 | time 0[s] | loss 1.54\n",
            "| epoch 198 | iter 1 / 1 | time 0[s] | loss 1.53\n",
            "| epoch 199 | iter 1 / 1 | time 0[s] | loss 1.55\n",
            "| epoch 200 | iter 1 / 1 | time 0[s] | loss 1.52\n",
            "| epoch 201 | iter 1 / 1 | time 0[s] | loss 1.51\n",
            "| epoch 202 | iter 1 / 1 | time 0[s] | loss 1.50\n",
            "| epoch 203 | iter 1 / 1 | time 0[s] | loss 1.54\n",
            "| epoch 204 | iter 1 / 1 | time 0[s] | loss 1.52\n",
            "| epoch 205 | iter 1 / 1 | time 0[s] | loss 1.51\n",
            "| epoch 206 | iter 1 / 1 | time 0[s] | loss 1.51\n",
            "| epoch 207 | iter 1 / 1 | time 0[s] | loss 1.50\n",
            "| epoch 208 | iter 1 / 1 | time 0[s] | loss 1.52\n",
            "| epoch 209 | iter 1 / 1 | time 0[s] | loss 1.51\n",
            "| epoch 210 | iter 1 / 1 | time 0[s] | loss 1.47\n",
            "| epoch 211 | iter 1 / 1 | time 0[s] | loss 1.48\n",
            "| epoch 212 | iter 1 / 1 | time 0[s] | loss 1.47\n",
            "| epoch 213 | iter 1 / 1 | time 0[s] | loss 1.48\n",
            "| epoch 214 | iter 1 / 1 | time 0[s] | loss 1.50\n",
            "| epoch 215 | iter 1 / 1 | time 0[s] | loss 1.49\n",
            "| epoch 216 | iter 1 / 1 | time 0[s] | loss 1.51\n",
            "| epoch 217 | iter 1 / 1 | time 0[s] | loss 1.48\n",
            "| epoch 218 | iter 1 / 1 | time 0[s] | loss 1.47\n",
            "| epoch 219 | iter 1 / 1 | time 0[s] | loss 1.46\n",
            "| epoch 220 | iter 1 / 1 | time 0[s] | loss 1.48\n",
            "| epoch 221 | iter 1 / 1 | time 0[s] | loss 1.44\n",
            "| epoch 222 | iter 1 / 1 | time 0[s] | loss 1.47\n",
            "| epoch 223 | iter 1 / 1 | time 0[s] | loss 1.44\n",
            "| epoch 224 | iter 1 / 1 | time 0[s] | loss 1.44\n",
            "| epoch 225 | iter 1 / 1 | time 0[s] | loss 1.46\n",
            "| epoch 226 | iter 1 / 1 | time 0[s] | loss 1.44\n",
            "| epoch 227 | iter 1 / 1 | time 0[s] | loss 1.48\n",
            "| epoch 228 | iter 1 / 1 | time 0[s] | loss 1.46\n",
            "| epoch 229 | iter 1 / 1 | time 0[s] | loss 1.43\n",
            "| epoch 230 | iter 1 / 1 | time 0[s] | loss 1.41\n",
            "| epoch 231 | iter 1 / 1 | time 0[s] | loss 1.45\n",
            "| epoch 232 | iter 1 / 1 | time 0[s] | loss 1.42\n",
            "| epoch 233 | iter 1 / 1 | time 0[s] | loss 1.44\n",
            "| epoch 234 | iter 1 / 1 | time 0[s] | loss 1.43\n",
            "| epoch 235 | iter 1 / 1 | time 0[s] | loss 1.43\n",
            "| epoch 236 | iter 1 / 1 | time 0[s] | loss 1.40\n",
            "| epoch 237 | iter 1 / 1 | time 0[s] | loss 1.43\n",
            "| epoch 238 | iter 1 / 1 | time 0[s] | loss 1.42\n",
            "| epoch 239 | iter 1 / 1 | time 0[s] | loss 1.39\n",
            "| epoch 240 | iter 1 / 1 | time 0[s] | loss 1.45\n",
            "| epoch 241 | iter 1 / 1 | time 0[s] | loss 1.41\n",
            "| epoch 242 | iter 1 / 1 | time 0[s] | loss 1.37\n",
            "| epoch 243 | iter 1 / 1 | time 0[s] | loss 1.40\n",
            "| epoch 244 | iter 1 / 1 | time 0[s] | loss 1.41\n",
            "| epoch 245 | iter 1 / 1 | time 0[s] | loss 1.44\n",
            "| epoch 246 | iter 1 / 1 | time 0[s] | loss 1.36\n",
            "| epoch 247 | iter 1 / 1 | time 0[s] | loss 1.37\n",
            "| epoch 248 | iter 1 / 1 | time 0[s] | loss 1.36\n",
            "| epoch 249 | iter 1 / 1 | time 0[s] | loss 1.39\n",
            "| epoch 250 | iter 1 / 1 | time 0[s] | loss 1.43\n",
            "| epoch 251 | iter 1 / 1 | time 0[s] | loss 1.39\n",
            "| epoch 252 | iter 1 / 1 | time 0[s] | loss 1.35\n",
            "| epoch 253 | iter 1 / 1 | time 0[s] | loss 1.42\n",
            "| epoch 254 | iter 1 / 1 | time 0[s] | loss 1.34\n",
            "| epoch 255 | iter 1 / 1 | time 0[s] | loss 1.37\n",
            "| epoch 256 | iter 1 / 1 | time 0[s] | loss 1.38\n",
            "| epoch 257 | iter 1 / 1 | time 0[s] | loss 1.38\n",
            "| epoch 258 | iter 1 / 1 | time 0[s] | loss 1.37\n",
            "| epoch 259 | iter 1 / 1 | time 0[s] | loss 1.32\n",
            "| epoch 260 | iter 1 / 1 | time 0[s] | loss 1.37\n",
            "| epoch 261 | iter 1 / 1 | time 0[s] | loss 1.37\n",
            "| epoch 262 | iter 1 / 1 | time 0[s] | loss 1.31\n",
            "| epoch 263 | iter 1 / 1 | time 0[s] | loss 1.36\n",
            "| epoch 264 | iter 1 / 1 | time 0[s] | loss 1.35\n",
            "| epoch 265 | iter 1 / 1 | time 0[s] | loss 1.30\n",
            "| epoch 266 | iter 1 / 1 | time 0[s] | loss 1.34\n",
            "| epoch 267 | iter 1 / 1 | time 0[s] | loss 1.34\n",
            "| epoch 268 | iter 1 / 1 | time 0[s] | loss 1.35\n",
            "| epoch 269 | iter 1 / 1 | time 0[s] | loss 1.34\n",
            "| epoch 270 | iter 1 / 1 | time 0[s] | loss 1.38\n",
            "| epoch 271 | iter 1 / 1 | time 0[s] | loss 1.33\n",
            "| epoch 272 | iter 1 / 1 | time 0[s] | loss 1.33\n",
            "| epoch 273 | iter 1 / 1 | time 0[s] | loss 1.28\n",
            "| epoch 274 | iter 1 / 1 | time 0[s] | loss 1.28\n",
            "| epoch 275 | iter 1 / 1 | time 0[s] | loss 1.37\n",
            "| epoch 276 | iter 1 / 1 | time 0[s] | loss 1.32\n",
            "| epoch 277 | iter 1 / 1 | time 0[s] | loss 1.28\n",
            "| epoch 278 | iter 1 / 1 | time 0[s] | loss 1.36\n",
            "| epoch 279 | iter 1 / 1 | time 0[s] | loss 1.27\n",
            "| epoch 280 | iter 1 / 1 | time 0[s] | loss 1.26\n",
            "| epoch 281 | iter 1 / 1 | time 0[s] | loss 1.31\n",
            "| epoch 282 | iter 1 / 1 | time 0[s] | loss 1.25\n",
            "| epoch 283 | iter 1 / 1 | time 0[s] | loss 1.31\n",
            "| epoch 284 | iter 1 / 1 | time 0[s] | loss 1.25\n",
            "| epoch 285 | iter 1 / 1 | time 0[s] | loss 1.35\n",
            "| epoch 286 | iter 1 / 1 | time 0[s] | loss 1.29\n",
            "| epoch 287 | iter 1 / 1 | time 0[s] | loss 1.30\n",
            "| epoch 288 | iter 1 / 1 | time 0[s] | loss 1.23\n",
            "| epoch 289 | iter 1 / 1 | time 0[s] | loss 1.24\n",
            "| epoch 290 | iter 1 / 1 | time 0[s] | loss 1.34\n",
            "| epoch 291 | iter 1 / 1 | time 0[s] | loss 1.23\n",
            "| epoch 292 | iter 1 / 1 | time 0[s] | loss 1.28\n",
            "| epoch 293 | iter 1 / 1 | time 0[s] | loss 1.27\n",
            "| epoch 294 | iter 1 / 1 | time 0[s] | loss 1.21\n",
            "| epoch 295 | iter 1 / 1 | time 0[s] | loss 1.22\n",
            "| epoch 296 | iter 1 / 1 | time 0[s] | loss 1.28\n",
            "| epoch 297 | iter 1 / 1 | time 0[s] | loss 1.27\n",
            "| epoch 298 | iter 1 / 1 | time 0[s] | loss 1.26\n",
            "| epoch 299 | iter 1 / 1 | time 0[s] | loss 1.25\n",
            "| epoch 300 | iter 1 / 1 | time 0[s] | loss 1.20\n",
            "| epoch 301 | iter 1 / 1 | time 0[s] | loss 1.19\n",
            "| epoch 302 | iter 1 / 1 | time 0[s] | loss 1.19\n",
            "| epoch 303 | iter 1 / 1 | time 0[s] | loss 1.18\n",
            "| epoch 304 | iter 1 / 1 | time 0[s] | loss 1.25\n",
            "| epoch 305 | iter 1 / 1 | time 0[s] | loss 1.31\n",
            "| epoch 306 | iter 1 / 1 | time 0[s] | loss 1.17\n",
            "| epoch 307 | iter 1 / 1 | time 0[s] | loss 1.17\n",
            "| epoch 308 | iter 1 / 1 | time 0[s] | loss 1.17\n",
            "| epoch 309 | iter 1 / 1 | time 0[s] | loss 1.24\n",
            "| epoch 310 | iter 1 / 1 | time 0[s] | loss 1.23\n",
            "| epoch 311 | iter 1 / 1 | time 0[s] | loss 1.23\n",
            "| epoch 312 | iter 1 / 1 | time 0[s] | loss 1.16\n",
            "| epoch 313 | iter 1 / 1 | time 0[s] | loss 1.23\n",
            "| epoch 314 | iter 1 / 1 | time 0[s] | loss 1.23\n",
            "| epoch 315 | iter 1 / 1 | time 0[s] | loss 1.29\n",
            "| epoch 316 | iter 1 / 1 | time 0[s] | loss 1.23\n",
            "| epoch 317 | iter 1 / 1 | time 0[s] | loss 1.21\n",
            "| epoch 318 | iter 1 / 1 | time 0[s] | loss 1.29\n",
            "| epoch 319 | iter 1 / 1 | time 0[s] | loss 1.14\n",
            "| epoch 320 | iter 1 / 1 | time 0[s] | loss 1.12\n",
            "| epoch 321 | iter 1 / 1 | time 0[s] | loss 1.12\n",
            "| epoch 322 | iter 1 / 1 | time 0[s] | loss 1.13\n",
            "| epoch 323 | iter 1 / 1 | time 0[s] | loss 1.11\n",
            "| epoch 324 | iter 1 / 1 | time 0[s] | loss 1.28\n",
            "| epoch 325 | iter 1 / 1 | time 0[s] | loss 1.12\n",
            "| epoch 326 | iter 1 / 1 | time 0[s] | loss 1.18\n",
            "| epoch 327 | iter 1 / 1 | time 0[s] | loss 1.21\n",
            "| epoch 328 | iter 1 / 1 | time 0[s] | loss 1.19\n",
            "| epoch 329 | iter 1 / 1 | time 0[s] | loss 1.19\n",
            "| epoch 330 | iter 1 / 1 | time 0[s] | loss 1.11\n",
            "| epoch 331 | iter 1 / 1 | time 0[s] | loss 1.10\n",
            "| epoch 332 | iter 1 / 1 | time 0[s] | loss 1.09\n",
            "| epoch 333 | iter 1 / 1 | time 0[s] | loss 1.10\n",
            "| epoch 334 | iter 1 / 1 | time 0[s] | loss 1.08\n",
            "| epoch 335 | iter 1 / 1 | time 0[s] | loss 1.11\n",
            "| epoch 336 | iter 1 / 1 | time 0[s] | loss 1.19\n",
            "| epoch 337 | iter 1 / 1 | time 0[s] | loss 1.18\n",
            "| epoch 338 | iter 1 / 1 | time 0[s] | loss 1.15\n",
            "| epoch 339 | iter 1 / 1 | time 0[s] | loss 1.25\n",
            "| epoch 340 | iter 1 / 1 | time 0[s] | loss 1.18\n",
            "| epoch 341 | iter 1 / 1 | time 0[s] | loss 1.25\n",
            "| epoch 342 | iter 1 / 1 | time 0[s] | loss 1.05\n",
            "| epoch 343 | iter 1 / 1 | time 0[s] | loss 1.18\n",
            "| epoch 344 | iter 1 / 1 | time 0[s] | loss 1.13\n",
            "| epoch 345 | iter 1 / 1 | time 0[s] | loss 1.17\n",
            "| epoch 346 | iter 1 / 1 | time 0[s] | loss 1.04\n",
            "| epoch 347 | iter 1 / 1 | time 0[s] | loss 1.08\n",
            "| epoch 348 | iter 1 / 1 | time 0[s] | loss 1.12\n",
            "| epoch 349 | iter 1 / 1 | time 0[s] | loss 1.08\n",
            "| epoch 350 | iter 1 / 1 | time 0[s] | loss 1.03\n",
            "| epoch 351 | iter 1 / 1 | time 0[s] | loss 1.04\n",
            "| epoch 352 | iter 1 / 1 | time 0[s] | loss 1.07\n",
            "| epoch 353 | iter 1 / 1 | time 0[s] | loss 1.11\n",
            "| epoch 354 | iter 1 / 1 | time 0[s] | loss 1.14\n",
            "| epoch 355 | iter 1 / 1 | time 0[s] | loss 1.10\n",
            "| epoch 356 | iter 1 / 1 | time 0[s] | loss 1.06\n",
            "| epoch 357 | iter 1 / 1 | time 0[s] | loss 1.14\n",
            "| epoch 358 | iter 1 / 1 | time 0[s] | loss 1.05\n",
            "| epoch 359 | iter 1 / 1 | time 0[s] | loss 1.02\n",
            "| epoch 360 | iter 1 / 1 | time 0[s] | loss 1.09\n",
            "| epoch 361 | iter 1 / 1 | time 0[s] | loss 1.22\n",
            "| epoch 362 | iter 1 / 1 | time 0[s] | loss 1.01\n",
            "| epoch 363 | iter 1 / 1 | time 0[s] | loss 1.04\n",
            "| epoch 364 | iter 1 / 1 | time 0[s] | loss 1.09\n",
            "| epoch 365 | iter 1 / 1 | time 0[s] | loss 0.99\n",
            "| epoch 366 | iter 1 / 1 | time 0[s] | loss 0.98\n",
            "| epoch 367 | iter 1 / 1 | time 0[s] | loss 1.22\n",
            "| epoch 368 | iter 1 / 1 | time 0[s] | loss 0.98\n",
            "| epoch 369 | iter 1 / 1 | time 0[s] | loss 1.07\n",
            "| epoch 370 | iter 1 / 1 | time 0[s] | loss 1.21\n",
            "| epoch 371 | iter 1 / 1 | time 0[s] | loss 1.02\n",
            "| epoch 372 | iter 1 / 1 | time 0[s] | loss 1.07\n",
            "| epoch 373 | iter 1 / 1 | time 0[s] | loss 1.02\n",
            "| epoch 374 | iter 1 / 1 | time 0[s] | loss 1.02\n",
            "| epoch 375 | iter 1 / 1 | time 0[s] | loss 1.06\n",
            "| epoch 376 | iter 1 / 1 | time 0[s] | loss 1.01\n",
            "| epoch 377 | iter 1 / 1 | time 0[s] | loss 1.06\n",
            "| epoch 378 | iter 1 / 1 | time 0[s] | loss 1.05\n",
            "| epoch 379 | iter 1 / 1 | time 0[s] | loss 1.05\n",
            "| epoch 380 | iter 1 / 1 | time 0[s] | loss 1.11\n",
            "| epoch 381 | iter 1 / 1 | time 0[s] | loss 1.09\n",
            "| epoch 382 | iter 1 / 1 | time 0[s] | loss 0.95\n",
            "| epoch 383 | iter 1 / 1 | time 0[s] | loss 1.00\n",
            "| epoch 384 | iter 1 / 1 | time 0[s] | loss 1.09\n",
            "| epoch 385 | iter 1 / 1 | time 0[s] | loss 1.04\n",
            "| epoch 386 | iter 1 / 1 | time 0[s] | loss 0.94\n",
            "| epoch 387 | iter 1 / 1 | time 0[s] | loss 1.08\n",
            "| epoch 388 | iter 1 / 1 | time 0[s] | loss 1.08\n",
            "| epoch 389 | iter 1 / 1 | time 0[s] | loss 1.08\n",
            "| epoch 390 | iter 1 / 1 | time 0[s] | loss 1.19\n",
            "| epoch 391 | iter 1 / 1 | time 0[s] | loss 1.09\n",
            "| epoch 392 | iter 1 / 1 | time 0[s] | loss 1.07\n",
            "| epoch 393 | iter 1 / 1 | time 0[s] | loss 1.02\n",
            "| epoch 394 | iter 1 / 1 | time 0[s] | loss 1.07\n",
            "| epoch 395 | iter 1 / 1 | time 0[s] | loss 1.07\n",
            "| epoch 396 | iter 1 / 1 | time 0[s] | loss 1.01\n",
            "| epoch 397 | iter 1 / 1 | time 0[s] | loss 0.92\n",
            "| epoch 398 | iter 1 / 1 | time 0[s] | loss 1.08\n",
            "| epoch 399 | iter 1 / 1 | time 0[s] | loss 1.01\n",
            "| epoch 400 | iter 1 / 1 | time 0[s] | loss 1.01\n",
            "| epoch 401 | iter 1 / 1 | time 0[s] | loss 1.08\n",
            "| epoch 402 | iter 1 / 1 | time 0[s] | loss 1.08\n",
            "| epoch 403 | iter 1 / 1 | time 0[s] | loss 0.96\n",
            "| epoch 404 | iter 1 / 1 | time 0[s] | loss 1.05\n",
            "| epoch 405 | iter 1 / 1 | time 0[s] | loss 1.00\n",
            "| epoch 406 | iter 1 / 1 | time 0[s] | loss 0.96\n",
            "| epoch 407 | iter 1 / 1 | time 0[s] | loss 0.90\n",
            "| epoch 408 | iter 1 / 1 | time 0[s] | loss 0.90\n",
            "| epoch 409 | iter 1 / 1 | time 0[s] | loss 0.99\n",
            "| epoch 410 | iter 1 / 1 | time 0[s] | loss 1.07\n",
            "| epoch 411 | iter 1 / 1 | time 0[s] | loss 0.95\n",
            "| epoch 412 | iter 1 / 1 | time 0[s] | loss 1.04\n",
            "| epoch 413 | iter 1 / 1 | time 0[s] | loss 1.06\n",
            "| epoch 414 | iter 1 / 1 | time 0[s] | loss 1.04\n",
            "| epoch 415 | iter 1 / 1 | time 0[s] | loss 0.88\n",
            "| epoch 416 | iter 1 / 1 | time 0[s] | loss 1.15\n",
            "| epoch 417 | iter 1 / 1 | time 0[s] | loss 1.04\n",
            "| epoch 418 | iter 1 / 1 | time 0[s] | loss 0.87\n",
            "| epoch 419 | iter 1 / 1 | time 0[s] | loss 0.97\n",
            "| epoch 420 | iter 1 / 1 | time 0[s] | loss 1.15\n",
            "| epoch 421 | iter 1 / 1 | time 0[s] | loss 0.87\n",
            "| epoch 422 | iter 1 / 1 | time 0[s] | loss 0.97\n",
            "| epoch 423 | iter 1 / 1 | time 0[s] | loss 1.03\n",
            "| epoch 424 | iter 1 / 1 | time 0[s] | loss 1.04\n",
            "| epoch 425 | iter 1 / 1 | time 0[s] | loss 0.93\n",
            "| epoch 426 | iter 1 / 1 | time 0[s] | loss 0.86\n",
            "| epoch 427 | iter 1 / 1 | time 0[s] | loss 1.04\n",
            "| epoch 428 | iter 1 / 1 | time 0[s] | loss 0.85\n",
            "| epoch 429 | iter 1 / 1 | time 0[s] | loss 1.14\n",
            "| epoch 430 | iter 1 / 1 | time 0[s] | loss 1.03\n",
            "| epoch 431 | iter 1 / 1 | time 0[s] | loss 1.03\n",
            "| epoch 432 | iter 1 / 1 | time 0[s] | loss 1.03\n",
            "| epoch 433 | iter 1 / 1 | time 0[s] | loss 0.84\n",
            "| epoch 434 | iter 1 / 1 | time 0[s] | loss 1.02\n",
            "| epoch 435 | iter 1 / 1 | time 0[s] | loss 1.02\n",
            "| epoch 436 | iter 1 / 1 | time 0[s] | loss 0.94\n",
            "| epoch 437 | iter 1 / 1 | time 0[s] | loss 1.13\n",
            "| epoch 438 | iter 1 / 1 | time 0[s] | loss 1.13\n",
            "| epoch 439 | iter 1 / 1 | time 0[s] | loss 1.02\n",
            "| epoch 440 | iter 1 / 1 | time 0[s] | loss 0.83\n",
            "| epoch 441 | iter 1 / 1 | time 0[s] | loss 1.01\n",
            "| epoch 442 | iter 1 / 1 | time 0[s] | loss 1.01\n",
            "| epoch 443 | iter 1 / 1 | time 0[s] | loss 0.90\n",
            "| epoch 444 | iter 1 / 1 | time 0[s] | loss 0.90\n",
            "| epoch 445 | iter 1 / 1 | time 0[s] | loss 1.00\n",
            "| epoch 446 | iter 1 / 1 | time 0[s] | loss 1.11\n",
            "| epoch 447 | iter 1 / 1 | time 0[s] | loss 0.90\n",
            "| epoch 448 | iter 1 / 1 | time 0[s] | loss 1.00\n",
            "| epoch 449 | iter 1 / 1 | time 0[s] | loss 1.01\n",
            "| epoch 450 | iter 1 / 1 | time 0[s] | loss 0.82\n",
            "| epoch 451 | iter 1 / 1 | time 0[s] | loss 0.93\n",
            "| epoch 452 | iter 1 / 1 | time 0[s] | loss 0.81\n",
            "| epoch 453 | iter 1 / 1 | time 0[s] | loss 1.00\n",
            "| epoch 454 | iter 1 / 1 | time 0[s] | loss 0.81\n",
            "| epoch 455 | iter 1 / 1 | time 0[s] | loss 0.92\n",
            "| epoch 456 | iter 1 / 1 | time 0[s] | loss 0.89\n",
            "| epoch 457 | iter 1 / 1 | time 0[s] | loss 0.99\n",
            "| epoch 458 | iter 1 / 1 | time 0[s] | loss 0.91\n",
            "| epoch 459 | iter 1 / 1 | time 0[s] | loss 0.81\n",
            "| epoch 460 | iter 1 / 1 | time 0[s] | loss 0.99\n",
            "| epoch 461 | iter 1 / 1 | time 0[s] | loss 1.00\n",
            "| epoch 462 | iter 1 / 1 | time 0[s] | loss 0.88\n",
            "| epoch 463 | iter 1 / 1 | time 0[s] | loss 0.79\n",
            "| epoch 464 | iter 1 / 1 | time 0[s] | loss 1.10\n",
            "| epoch 465 | iter 1 / 1 | time 0[s] | loss 0.90\n",
            "| epoch 466 | iter 1 / 1 | time 0[s] | loss 0.99\n",
            "| epoch 467 | iter 1 / 1 | time 0[s] | loss 0.90\n",
            "| epoch 468 | iter 1 / 1 | time 0[s] | loss 0.78\n",
            "| epoch 469 | iter 1 / 1 | time 0[s] | loss 0.98\n",
            "| epoch 470 | iter 1 / 1 | time 0[s] | loss 0.97\n",
            "| epoch 471 | iter 1 / 1 | time 0[s] | loss 0.98\n",
            "| epoch 472 | iter 1 / 1 | time 0[s] | loss 0.87\n",
            "| epoch 473 | iter 1 / 1 | time 0[s] | loss 0.97\n",
            "| epoch 474 | iter 1 / 1 | time 0[s] | loss 0.89\n",
            "| epoch 475 | iter 1 / 1 | time 0[s] | loss 0.97\n",
            "| epoch 476 | iter 1 / 1 | time 0[s] | loss 0.98\n",
            "| epoch 477 | iter 1 / 1 | time 0[s] | loss 0.98\n",
            "| epoch 478 | iter 1 / 1 | time 0[s] | loss 0.78\n",
            "| epoch 479 | iter 1 / 1 | time 0[s] | loss 0.86\n",
            "| epoch 480 | iter 1 / 1 | time 0[s] | loss 0.88\n",
            "| epoch 481 | iter 1 / 1 | time 0[s] | loss 0.76\n",
            "| epoch 482 | iter 1 / 1 | time 0[s] | loss 0.97\n",
            "| epoch 483 | iter 1 / 1 | time 0[s] | loss 0.76\n",
            "| epoch 484 | iter 1 / 1 | time 0[s] | loss 0.87\n",
            "| epoch 485 | iter 1 / 1 | time 0[s] | loss 1.08\n",
            "| epoch 486 | iter 1 / 1 | time 0[s] | loss 0.96\n",
            "| epoch 487 | iter 1 / 1 | time 0[s] | loss 0.76\n",
            "| epoch 488 | iter 1 / 1 | time 0[s] | loss 0.85\n",
            "| epoch 489 | iter 1 / 1 | time 0[s] | loss 0.96\n",
            "| epoch 490 | iter 1 / 1 | time 0[s] | loss 0.96\n",
            "| epoch 491 | iter 1 / 1 | time 0[s] | loss 0.85\n",
            "| epoch 492 | iter 1 / 1 | time 0[s] | loss 1.07\n",
            "| epoch 493 | iter 1 / 1 | time 0[s] | loss 0.97\n",
            "| epoch 494 | iter 1 / 1 | time 0[s] | loss 0.96\n",
            "| epoch 495 | iter 1 / 1 | time 0[s] | loss 0.84\n",
            "| epoch 496 | iter 1 / 1 | time 0[s] | loss 0.95\n",
            "| epoch 497 | iter 1 / 1 | time 0[s] | loss 0.86\n",
            "| epoch 498 | iter 1 / 1 | time 0[s] | loss 0.96\n",
            "| epoch 499 | iter 1 / 1 | time 0[s] | loss 0.86\n",
            "| epoch 500 | iter 1 / 1 | time 0[s] | loss 0.95\n",
            "| epoch 501 | iter 1 / 1 | time 0[s] | loss 1.06\n",
            "| epoch 502 | iter 1 / 1 | time 0[s] | loss 0.94\n",
            "| epoch 503 | iter 1 / 1 | time 0[s] | loss 0.94\n",
            "| epoch 504 | iter 1 / 1 | time 0[s] | loss 0.75\n",
            "| epoch 505 | iter 1 / 1 | time 0[s] | loss 0.73\n",
            "| epoch 506 | iter 1 / 1 | time 0[s] | loss 0.73\n",
            "| epoch 507 | iter 1 / 1 | time 0[s] | loss 0.85\n",
            "| epoch 508 | iter 1 / 1 | time 0[s] | loss 0.73\n",
            "| epoch 509 | iter 1 / 1 | time 0[s] | loss 0.74\n",
            "| epoch 510 | iter 1 / 1 | time 0[s] | loss 0.74\n",
            "| epoch 511 | iter 1 / 1 | time 0[s] | loss 0.85\n",
            "| epoch 512 | iter 1 / 1 | time 0[s] | loss 0.84\n",
            "| epoch 513 | iter 1 / 1 | time 0[s] | loss 0.95\n",
            "| epoch 514 | iter 1 / 1 | time 0[s] | loss 0.72\n",
            "| epoch 515 | iter 1 / 1 | time 0[s] | loss 1.06\n",
            "| epoch 516 | iter 1 / 1 | time 0[s] | loss 0.84\n",
            "| epoch 517 | iter 1 / 1 | time 0[s] | loss 0.71\n",
            "| epoch 518 | iter 1 / 1 | time 0[s] | loss 0.83\n",
            "| epoch 519 | iter 1 / 1 | time 0[s] | loss 0.83\n",
            "| epoch 520 | iter 1 / 1 | time 0[s] | loss 0.70\n",
            "| epoch 521 | iter 1 / 1 | time 0[s] | loss 0.93\n",
            "| epoch 522 | iter 1 / 1 | time 0[s] | loss 0.83\n",
            "| epoch 523 | iter 1 / 1 | time 0[s] | loss 0.83\n",
            "| epoch 524 | iter 1 / 1 | time 0[s] | loss 0.93\n",
            "| epoch 525 | iter 1 / 1 | time 0[s] | loss 0.69\n",
            "| epoch 526 | iter 1 / 1 | time 0[s] | loss 0.82\n",
            "| epoch 527 | iter 1 / 1 | time 0[s] | loss 0.95\n",
            "| epoch 528 | iter 1 / 1 | time 0[s] | loss 1.05\n",
            "| epoch 529 | iter 1 / 1 | time 0[s] | loss 0.68\n",
            "| epoch 530 | iter 1 / 1 | time 0[s] | loss 0.82\n",
            "| epoch 531 | iter 1 / 1 | time 0[s] | loss 0.81\n",
            "| epoch 532 | iter 1 / 1 | time 0[s] | loss 0.81\n",
            "| epoch 533 | iter 1 / 1 | time 0[s] | loss 0.81\n",
            "| epoch 534 | iter 1 / 1 | time 0[s] | loss 0.91\n",
            "| epoch 535 | iter 1 / 1 | time 0[s] | loss 0.70\n",
            "| epoch 536 | iter 1 / 1 | time 0[s] | loss 0.95\n",
            "| epoch 537 | iter 1 / 1 | time 0[s] | loss 0.95\n",
            "| epoch 538 | iter 1 / 1 | time 0[s] | loss 0.81\n",
            "| epoch 539 | iter 1 / 1 | time 0[s] | loss 0.92\n",
            "| epoch 540 | iter 1 / 1 | time 0[s] | loss 0.67\n",
            "| epoch 541 | iter 1 / 1 | time 0[s] | loss 0.66\n",
            "| epoch 542 | iter 1 / 1 | time 0[s] | loss 0.69\n",
            "| epoch 543 | iter 1 / 1 | time 0[s] | loss 1.05\n",
            "| epoch 544 | iter 1 / 1 | time 0[s] | loss 0.69\n",
            "| epoch 545 | iter 1 / 1 | time 0[s] | loss 0.80\n",
            "| epoch 546 | iter 1 / 1 | time 0[s] | loss 0.91\n",
            "| epoch 547 | iter 1 / 1 | time 0[s] | loss 0.91\n",
            "| epoch 548 | iter 1 / 1 | time 0[s] | loss 0.80\n",
            "| epoch 549 | iter 1 / 1 | time 0[s] | loss 0.91\n",
            "| epoch 550 | iter 1 / 1 | time 0[s] | loss 0.68\n",
            "| epoch 551 | iter 1 / 1 | time 0[s] | loss 0.94\n",
            "| epoch 552 | iter 1 / 1 | time 0[s] | loss 1.05\n",
            "| epoch 553 | iter 1 / 1 | time 0[s] | loss 0.91\n",
            "| epoch 554 | iter 1 / 1 | time 0[s] | loss 0.90\n",
            "| epoch 555 | iter 1 / 1 | time 0[s] | loss 0.90\n",
            "| epoch 556 | iter 1 / 1 | time 0[s] | loss 0.79\n",
            "| epoch 557 | iter 1 / 1 | time 0[s] | loss 0.93\n",
            "| epoch 558 | iter 1 / 1 | time 0[s] | loss 0.79\n",
            "| epoch 559 | iter 1 / 1 | time 0[s] | loss 0.94\n",
            "| epoch 560 | iter 1 / 1 | time 0[s] | loss 0.78\n",
            "| epoch 561 | iter 1 / 1 | time 0[s] | loss 0.79\n",
            "| epoch 562 | iter 1 / 1 | time 0[s] | loss 0.90\n",
            "| epoch 563 | iter 1 / 1 | time 0[s] | loss 0.79\n",
            "| epoch 564 | iter 1 / 1 | time 0[s] | loss 1.04\n",
            "| epoch 565 | iter 1 / 1 | time 0[s] | loss 0.64\n",
            "| epoch 566 | iter 1 / 1 | time 0[s] | loss 0.89\n",
            "| epoch 567 | iter 1 / 1 | time 0[s] | loss 0.89\n",
            "| epoch 568 | iter 1 / 1 | time 0[s] | loss 0.68\n",
            "| epoch 569 | iter 1 / 1 | time 0[s] | loss 1.03\n",
            "| epoch 570 | iter 1 / 1 | time 0[s] | loss 1.03\n",
            "| epoch 571 | iter 1 / 1 | time 0[s] | loss 0.64\n",
            "| epoch 572 | iter 1 / 1 | time 0[s] | loss 0.79\n",
            "| epoch 573 | iter 1 / 1 | time 0[s] | loss 0.78\n",
            "| epoch 574 | iter 1 / 1 | time 0[s] | loss 0.93\n",
            "| epoch 575 | iter 1 / 1 | time 0[s] | loss 0.78\n",
            "| epoch 576 | iter 1 / 1 | time 0[s] | loss 0.67\n",
            "| epoch 577 | iter 1 / 1 | time 0[s] | loss 0.77\n",
            "| epoch 578 | iter 1 / 1 | time 0[s] | loss 0.77\n",
            "| epoch 579 | iter 1 / 1 | time 0[s] | loss 0.77\n",
            "| epoch 580 | iter 1 / 1 | time 0[s] | loss 0.92\n",
            "| epoch 581 | iter 1 / 1 | time 0[s] | loss 0.92\n",
            "| epoch 582 | iter 1 / 1 | time 0[s] | loss 0.77\n",
            "| epoch 583 | iter 1 / 1 | time 0[s] | loss 0.92\n",
            "| epoch 584 | iter 1 / 1 | time 0[s] | loss 0.78\n",
            "| epoch 585 | iter 1 / 1 | time 0[s] | loss 0.92\n",
            "| epoch 586 | iter 1 / 1 | time 0[s] | loss 1.02\n",
            "| epoch 587 | iter 1 / 1 | time 0[s] | loss 0.92\n",
            "| epoch 588 | iter 1 / 1 | time 0[s] | loss 0.91\n",
            "| epoch 589 | iter 1 / 1 | time 0[s] | loss 0.88\n",
            "| epoch 590 | iter 1 / 1 | time 0[s] | loss 0.91\n",
            "| epoch 591 | iter 1 / 1 | time 0[s] | loss 0.76\n",
            "| epoch 592 | iter 1 / 1 | time 0[s] | loss 0.88\n",
            "| epoch 593 | iter 1 / 1 | time 0[s] | loss 0.88\n",
            "| epoch 594 | iter 1 / 1 | time 0[s] | loss 0.88\n",
            "| epoch 595 | iter 1 / 1 | time 0[s] | loss 0.90\n",
            "| epoch 596 | iter 1 / 1 | time 0[s] | loss 0.63\n",
            "| epoch 597 | iter 1 / 1 | time 0[s] | loss 0.66\n",
            "| epoch 598 | iter 1 / 1 | time 0[s] | loss 1.01\n",
            "| epoch 599 | iter 1 / 1 | time 0[s] | loss 0.63\n",
            "| epoch 600 | iter 1 / 1 | time 0[s] | loss 1.01\n",
            "| epoch 601 | iter 1 / 1 | time 0[s] | loss 1.01\n",
            "| epoch 602 | iter 1 / 1 | time 0[s] | loss 1.01\n",
            "| epoch 603 | iter 1 / 1 | time 0[s] | loss 0.76\n",
            "| epoch 604 | iter 1 / 1 | time 0[s] | loss 1.00\n",
            "| epoch 605 | iter 1 / 1 | time 0[s] | loss 0.77\n",
            "| epoch 606 | iter 1 / 1 | time 0[s] | loss 0.76\n",
            "| epoch 607 | iter 1 / 1 | time 0[s] | loss 0.63\n",
            "| epoch 608 | iter 1 / 1 | time 0[s] | loss 0.90\n",
            "| epoch 609 | iter 1 / 1 | time 0[s] | loss 0.75\n",
            "| epoch 610 | iter 1 / 1 | time 0[s] | loss 0.65\n",
            "| epoch 611 | iter 1 / 1 | time 0[s] | loss 0.87\n",
            "| epoch 612 | iter 1 / 1 | time 0[s] | loss 0.87\n",
            "| epoch 613 | iter 1 / 1 | time 0[s] | loss 0.89\n",
            "| epoch 614 | iter 1 / 1 | time 0[s] | loss 0.75\n",
            "| epoch 615 | iter 1 / 1 | time 0[s] | loss 0.65\n",
            "| epoch 616 | iter 1 / 1 | time 0[s] | loss 0.65\n",
            "| epoch 617 | iter 1 / 1 | time 0[s] | loss 0.65\n",
            "| epoch 618 | iter 1 / 1 | time 0[s] | loss 0.75\n",
            "| epoch 619 | iter 1 / 1 | time 0[s] | loss 0.76\n",
            "| epoch 620 | iter 1 / 1 | time 0[s] | loss 0.89\n",
            "| epoch 621 | iter 1 / 1 | time 0[s] | loss 0.74\n",
            "| epoch 622 | iter 1 / 1 | time 0[s] | loss 0.89\n",
            "| epoch 623 | iter 1 / 1 | time 0[s] | loss 0.87\n",
            "| epoch 624 | iter 1 / 1 | time 0[s] | loss 0.76\n",
            "| epoch 625 | iter 1 / 1 | time 0[s] | loss 0.76\n",
            "| epoch 626 | iter 1 / 1 | time 0[s] | loss 0.87\n",
            "| epoch 627 | iter 1 / 1 | time 0[s] | loss 0.88\n",
            "| epoch 628 | iter 1 / 1 | time 0[s] | loss 0.87\n",
            "| epoch 629 | iter 1 / 1 | time 0[s] | loss 0.62\n",
            "| epoch 630 | iter 1 / 1 | time 0[s] | loss 0.74\n",
            "| epoch 631 | iter 1 / 1 | time 0[s] | loss 0.76\n",
            "| epoch 632 | iter 1 / 1 | time 0[s] | loss 0.63\n",
            "| epoch 633 | iter 1 / 1 | time 0[s] | loss 0.87\n",
            "| epoch 634 | iter 1 / 1 | time 0[s] | loss 0.74\n",
            "| epoch 635 | iter 1 / 1 | time 0[s] | loss 0.99\n",
            "| epoch 636 | iter 1 / 1 | time 0[s] | loss 0.61\n",
            "| epoch 637 | iter 1 / 1 | time 0[s] | loss 0.88\n",
            "| epoch 638 | iter 1 / 1 | time 0[s] | loss 0.88\n",
            "| epoch 639 | iter 1 / 1 | time 0[s] | loss 0.74\n",
            "| epoch 640 | iter 1 / 1 | time 0[s] | loss 0.86\n",
            "| epoch 641 | iter 1 / 1 | time 0[s] | loss 0.86\n",
            "| epoch 642 | iter 1 / 1 | time 0[s] | loss 0.61\n",
            "| epoch 643 | iter 1 / 1 | time 0[s] | loss 0.88\n",
            "| epoch 644 | iter 1 / 1 | time 0[s] | loss 0.86\n",
            "| epoch 645 | iter 1 / 1 | time 0[s] | loss 0.88\n",
            "| epoch 646 | iter 1 / 1 | time 0[s] | loss 0.60\n",
            "| epoch 647 | iter 1 / 1 | time 0[s] | loss 0.75\n",
            "| epoch 648 | iter 1 / 1 | time 0[s] | loss 0.73\n",
            "| epoch 649 | iter 1 / 1 | time 0[s] | loss 0.62\n",
            "| epoch 650 | iter 1 / 1 | time 0[s] | loss 0.86\n",
            "| epoch 651 | iter 1 / 1 | time 0[s] | loss 0.87\n",
            "| epoch 652 | iter 1 / 1 | time 0[s] | loss 0.60\n",
            "| epoch 653 | iter 1 / 1 | time 0[s] | loss 0.75\n",
            "| epoch 654 | iter 1 / 1 | time 0[s] | loss 0.87\n",
            "| epoch 655 | iter 1 / 1 | time 0[s] | loss 0.73\n",
            "| epoch 656 | iter 1 / 1 | time 0[s] | loss 0.98\n",
            "| epoch 657 | iter 1 / 1 | time 0[s] | loss 0.74\n",
            "| epoch 658 | iter 1 / 1 | time 0[s] | loss 0.60\n",
            "| epoch 659 | iter 1 / 1 | time 0[s] | loss 0.61\n",
            "| epoch 660 | iter 1 / 1 | time 0[s] | loss 0.59\n",
            "| epoch 661 | iter 1 / 1 | time 0[s] | loss 0.61\n",
            "| epoch 662 | iter 1 / 1 | time 0[s] | loss 0.74\n",
            "| epoch 663 | iter 1 / 1 | time 0[s] | loss 0.87\n",
            "| epoch 664 | iter 1 / 1 | time 0[s] | loss 0.87\n",
            "| epoch 665 | iter 1 / 1 | time 0[s] | loss 0.59\n",
            "| epoch 666 | iter 1 / 1 | time 0[s] | loss 0.86\n",
            "| epoch 667 | iter 1 / 1 | time 0[s] | loss 0.85\n",
            "| epoch 668 | iter 1 / 1 | time 0[s] | loss 0.86\n",
            "| epoch 669 | iter 1 / 1 | time 0[s] | loss 0.74\n",
            "| epoch 670 | iter 1 / 1 | time 0[s] | loss 0.72\n",
            "| epoch 671 | iter 1 / 1 | time 0[s] | loss 0.71\n",
            "| epoch 672 | iter 1 / 1 | time 0[s] | loss 0.87\n",
            "| epoch 673 | iter 1 / 1 | time 0[s] | loss 0.72\n",
            "| epoch 674 | iter 1 / 1 | time 0[s] | loss 0.74\n",
            "| epoch 675 | iter 1 / 1 | time 0[s] | loss 0.58\n",
            "| epoch 676 | iter 1 / 1 | time 0[s] | loss 0.87\n",
            "| epoch 677 | iter 1 / 1 | time 0[s] | loss 0.71\n",
            "| epoch 678 | iter 1 / 1 | time 0[s] | loss 0.85\n",
            "| epoch 679 | iter 1 / 1 | time 0[s] | loss 0.87\n",
            "| epoch 680 | iter 1 / 1 | time 0[s] | loss 0.71\n",
            "| epoch 681 | iter 1 / 1 | time 0[s] | loss 0.71\n",
            "| epoch 682 | iter 1 / 1 | time 0[s] | loss 0.71\n",
            "| epoch 683 | iter 1 / 1 | time 0[s] | loss 0.87\n",
            "| epoch 684 | iter 1 / 1 | time 0[s] | loss 0.87\n",
            "| epoch 685 | iter 1 / 1 | time 0[s] | loss 0.59\n",
            "| epoch 686 | iter 1 / 1 | time 0[s] | loss 0.70\n",
            "| epoch 687 | iter 1 / 1 | time 0[s] | loss 0.71\n",
            "| epoch 688 | iter 1 / 1 | time 0[s] | loss 0.85\n",
            "| epoch 689 | iter 1 / 1 | time 0[s] | loss 0.57\n",
            "| epoch 690 | iter 1 / 1 | time 0[s] | loss 0.86\n",
            "| epoch 691 | iter 1 / 1 | time 0[s] | loss 0.58\n",
            "| epoch 692 | iter 1 / 1 | time 0[s] | loss 0.58\n",
            "| epoch 693 | iter 1 / 1 | time 0[s] | loss 0.70\n",
            "| epoch 694 | iter 1 / 1 | time 0[s] | loss 0.98\n",
            "| epoch 695 | iter 1 / 1 | time 0[s] | loss 0.58\n",
            "| epoch 696 | iter 1 / 1 | time 0[s] | loss 0.85\n",
            "| epoch 697 | iter 1 / 1 | time 0[s] | loss 0.57\n",
            "| epoch 698 | iter 1 / 1 | time 0[s] | loss 0.86\n",
            "| epoch 699 | iter 1 / 1 | time 0[s] | loss 0.85\n",
            "| epoch 700 | iter 1 / 1 | time 0[s] | loss 0.57\n",
            "| epoch 701 | iter 1 / 1 | time 0[s] | loss 0.69\n",
            "| epoch 702 | iter 1 / 1 | time 0[s] | loss 0.70\n",
            "| epoch 703 | iter 1 / 1 | time 0[s] | loss 0.98\n",
            "| epoch 704 | iter 1 / 1 | time 0[s] | loss 0.69\n",
            "| epoch 705 | iter 1 / 1 | time 0[s] | loss 0.56\n",
            "| epoch 706 | iter 1 / 1 | time 0[s] | loss 0.69\n",
            "| epoch 707 | iter 1 / 1 | time 0[s] | loss 0.85\n",
            "| epoch 708 | iter 1 / 1 | time 0[s] | loss 0.85\n",
            "| epoch 709 | iter 1 / 1 | time 0[s] | loss 0.69\n",
            "| epoch 710 | iter 1 / 1 | time 0[s] | loss 0.86\n",
            "| epoch 711 | iter 1 / 1 | time 0[s] | loss 0.86\n",
            "| epoch 712 | iter 1 / 1 | time 0[s] | loss 0.69\n",
            "| epoch 713 | iter 1 / 1 | time 0[s] | loss 0.69\n",
            "| epoch 714 | iter 1 / 1 | time 0[s] | loss 0.85\n",
            "| epoch 715 | iter 1 / 1 | time 0[s] | loss 0.55\n",
            "| epoch 716 | iter 1 / 1 | time 0[s] | loss 0.68\n",
            "| epoch 717 | iter 1 / 1 | time 0[s] | loss 0.68\n",
            "| epoch 718 | iter 1 / 1 | time 0[s] | loss 0.85\n",
            "| epoch 719 | iter 1 / 1 | time 0[s] | loss 0.55\n",
            "| epoch 720 | iter 1 / 1 | time 0[s] | loss 0.56\n",
            "| epoch 721 | iter 1 / 1 | time 0[s] | loss 0.72\n",
            "| epoch 722 | iter 1 / 1 | time 0[s] | loss 0.86\n",
            "| epoch 723 | iter 1 / 1 | time 0[s] | loss 0.68\n",
            "| epoch 724 | iter 1 / 1 | time 0[s] | loss 0.56\n",
            "| epoch 725 | iter 1 / 1 | time 0[s] | loss 0.98\n",
            "| epoch 726 | iter 1 / 1 | time 0[s] | loss 0.68\n",
            "| epoch 727 | iter 1 / 1 | time 0[s] | loss 0.86\n",
            "| epoch 728 | iter 1 / 1 | time 0[s] | loss 0.54\n",
            "| epoch 729 | iter 1 / 1 | time 0[s] | loss 0.68\n",
            "| epoch 730 | iter 1 / 1 | time 0[s] | loss 0.55\n",
            "| epoch 731 | iter 1 / 1 | time 0[s] | loss 0.86\n",
            "| epoch 732 | iter 1 / 1 | time 0[s] | loss 0.54\n",
            "| epoch 733 | iter 1 / 1 | time 0[s] | loss 0.54\n",
            "| epoch 734 | iter 1 / 1 | time 0[s] | loss 0.86\n",
            "| epoch 735 | iter 1 / 1 | time 0[s] | loss 0.67\n",
            "| epoch 736 | iter 1 / 1 | time 0[s] | loss 0.67\n",
            "| epoch 737 | iter 1 / 1 | time 0[s] | loss 0.86\n",
            "| epoch 738 | iter 1 / 1 | time 0[s] | loss 0.54\n",
            "| epoch 739 | iter 1 / 1 | time 0[s] | loss 0.85\n",
            "| epoch 740 | iter 1 / 1 | time 0[s] | loss 0.54\n",
            "| epoch 741 | iter 1 / 1 | time 0[s] | loss 0.86\n",
            "| epoch 742 | iter 1 / 1 | time 0[s] | loss 0.99\n",
            "| epoch 743 | iter 1 / 1 | time 0[s] | loss 0.67\n",
            "| epoch 744 | iter 1 / 1 | time 0[s] | loss 0.85\n",
            "| epoch 745 | iter 1 / 1 | time 0[s] | loss 0.86\n",
            "| epoch 746 | iter 1 / 1 | time 0[s] | loss 0.85\n",
            "| epoch 747 | iter 1 / 1 | time 0[s] | loss 0.85\n",
            "| epoch 748 | iter 1 / 1 | time 0[s] | loss 0.72\n",
            "| epoch 749 | iter 1 / 1 | time 0[s] | loss 0.85\n",
            "| epoch 750 | iter 1 / 1 | time 0[s] | loss 0.99\n",
            "| epoch 751 | iter 1 / 1 | time 0[s] | loss 0.85\n",
            "| epoch 752 | iter 1 / 1 | time 0[s] | loss 0.53\n",
            "| epoch 753 | iter 1 / 1 | time 0[s] | loss 0.85\n",
            "| epoch 754 | iter 1 / 1 | time 0[s] | loss 0.85\n",
            "| epoch 755 | iter 1 / 1 | time 0[s] | loss 0.85\n",
            "| epoch 756 | iter 1 / 1 | time 0[s] | loss 0.85\n",
            "| epoch 757 | iter 1 / 1 | time 0[s] | loss 0.67\n",
            "| epoch 758 | iter 1 / 1 | time 0[s] | loss 0.54\n",
            "| epoch 759 | iter 1 / 1 | time 0[s] | loss 0.98\n",
            "| epoch 760 | iter 1 / 1 | time 0[s] | loss 0.54\n",
            "| epoch 761 | iter 1 / 1 | time 0[s] | loss 0.54\n",
            "| epoch 762 | iter 1 / 1 | time 0[s] | loss 0.54\n",
            "| epoch 763 | iter 1 / 1 | time 0[s] | loss 0.84\n",
            "| epoch 764 | iter 1 / 1 | time 0[s] | loss 0.84\n",
            "| epoch 765 | iter 1 / 1 | time 0[s] | loss 0.84\n",
            "| epoch 766 | iter 1 / 1 | time 0[s] | loss 0.54\n",
            "| epoch 767 | iter 1 / 1 | time 0[s] | loss 0.53\n",
            "| epoch 768 | iter 1 / 1 | time 0[s] | loss 0.85\n",
            "| epoch 769 | iter 1 / 1 | time 0[s] | loss 0.71\n",
            "| epoch 770 | iter 1 / 1 | time 0[s] | loss 0.54\n",
            "| epoch 771 | iter 1 / 1 | time 0[s] | loss 0.97\n",
            "| epoch 772 | iter 1 / 1 | time 0[s] | loss 0.66\n",
            "| epoch 773 | iter 1 / 1 | time 0[s] | loss 0.84\n",
            "| epoch 774 | iter 1 / 1 | time 0[s] | loss 0.66\n",
            "| epoch 775 | iter 1 / 1 | time 0[s] | loss 0.97\n",
            "| epoch 776 | iter 1 / 1 | time 0[s] | loss 0.71\n",
            "| epoch 777 | iter 1 / 1 | time 0[s] | loss 0.84\n",
            "| epoch 778 | iter 1 / 1 | time 0[s] | loss 0.53\n",
            "| epoch 779 | iter 1 / 1 | time 0[s] | loss 0.84\n",
            "| epoch 780 | iter 1 / 1 | time 0[s] | loss 0.84\n",
            "| epoch 781 | iter 1 / 1 | time 0[s] | loss 0.53\n",
            "| epoch 782 | iter 1 / 1 | time 0[s] | loss 0.84\n",
            "| epoch 783 | iter 1 / 1 | time 0[s] | loss 0.53\n",
            "| epoch 784 | iter 1 / 1 | time 0[s] | loss 0.66\n",
            "| epoch 785 | iter 1 / 1 | time 0[s] | loss 0.71\n",
            "| epoch 786 | iter 1 / 1 | time 0[s] | loss 0.66\n",
            "| epoch 787 | iter 1 / 1 | time 0[s] | loss 0.66\n",
            "| epoch 788 | iter 1 / 1 | time 0[s] | loss 0.66\n",
            "| epoch 789 | iter 1 / 1 | time 0[s] | loss 0.66\n",
            "| epoch 790 | iter 1 / 1 | time 0[s] | loss 0.52\n",
            "| epoch 791 | iter 1 / 1 | time 0[s] | loss 0.97\n",
            "| epoch 792 | iter 1 / 1 | time 0[s] | loss 0.66\n",
            "| epoch 793 | iter 1 / 1 | time 0[s] | loss 0.83\n",
            "| epoch 794 | iter 1 / 1 | time 0[s] | loss 0.65\n",
            "| epoch 795 | iter 1 / 1 | time 0[s] | loss 0.97\n",
            "| epoch 796 | iter 1 / 1 | time 0[s] | loss 0.83\n",
            "| epoch 797 | iter 1 / 1 | time 0[s] | loss 0.65\n",
            "| epoch 798 | iter 1 / 1 | time 0[s] | loss 0.97\n",
            "| epoch 799 | iter 1 / 1 | time 0[s] | loss 0.51\n",
            "| epoch 800 | iter 1 / 1 | time 0[s] | loss 0.82\n",
            "| epoch 801 | iter 1 / 1 | time 0[s] | loss 0.85\n",
            "| epoch 802 | iter 1 / 1 | time 0[s] | loss 0.85\n",
            "| epoch 803 | iter 1 / 1 | time 0[s] | loss 0.97\n",
            "| epoch 804 | iter 1 / 1 | time 0[s] | loss 0.70\n",
            "| epoch 805 | iter 1 / 1 | time 0[s] | loss 0.85\n",
            "| epoch 806 | iter 1 / 1 | time 0[s] | loss 0.70\n",
            "| epoch 807 | iter 1 / 1 | time 0[s] | loss 0.85\n",
            "| epoch 808 | iter 1 / 1 | time 0[s] | loss 0.84\n",
            "| epoch 809 | iter 1 / 1 | time 0[s] | loss 0.51\n",
            "| epoch 810 | iter 1 / 1 | time 0[s] | loss 0.70\n",
            "| epoch 811 | iter 1 / 1 | time 0[s] | loss 0.51\n",
            "| epoch 812 | iter 1 / 1 | time 0[s] | loss 0.82\n",
            "| epoch 813 | iter 1 / 1 | time 0[s] | loss 0.70\n",
            "| epoch 814 | iter 1 / 1 | time 0[s] | loss 0.84\n",
            "| epoch 815 | iter 1 / 1 | time 0[s] | loss 0.84\n",
            "| epoch 816 | iter 1 / 1 | time 0[s] | loss 0.65\n",
            "| epoch 817 | iter 1 / 1 | time 0[s] | loss 0.81\n",
            "| epoch 818 | iter 1 / 1 | time 0[s] | loss 0.95\n",
            "| epoch 819 | iter 1 / 1 | time 0[s] | loss 0.84\n",
            "| epoch 820 | iter 1 / 1 | time 0[s] | loss 0.95\n",
            "| epoch 821 | iter 1 / 1 | time 0[s] | loss 0.84\n",
            "| epoch 822 | iter 1 / 1 | time 0[s] | loss 0.95\n",
            "| epoch 823 | iter 1 / 1 | time 0[s] | loss 0.65\n",
            "| epoch 824 | iter 1 / 1 | time 0[s] | loss 0.70\n",
            "| epoch 825 | iter 1 / 1 | time 0[s] | loss 0.81\n",
            "| epoch 826 | iter 1 / 1 | time 0[s] | loss 0.65\n",
            "| epoch 827 | iter 1 / 1 | time 0[s] | loss 0.54\n",
            "| epoch 828 | iter 1 / 1 | time 0[s] | loss 0.83\n",
            "| epoch 829 | iter 1 / 1 | time 0[s] | loss 0.66\n",
            "| epoch 830 | iter 1 / 1 | time 0[s] | loss 0.54\n",
            "| epoch 831 | iter 1 / 1 | time 0[s] | loss 0.94\n",
            "| epoch 832 | iter 1 / 1 | time 0[s] | loss 0.81\n",
            "| epoch 833 | iter 1 / 1 | time 0[s] | loss 0.65\n",
            "| epoch 834 | iter 1 / 1 | time 0[s] | loss 0.65\n",
            "| epoch 835 | iter 1 / 1 | time 0[s] | loss 0.81\n",
            "| epoch 836 | iter 1 / 1 | time 0[s] | loss 0.65\n",
            "| epoch 837 | iter 1 / 1 | time 0[s] | loss 0.65\n",
            "| epoch 838 | iter 1 / 1 | time 0[s] | loss 0.66\n",
            "| epoch 839 | iter 1 / 1 | time 0[s] | loss 0.65\n",
            "| epoch 840 | iter 1 / 1 | time 0[s] | loss 0.83\n",
            "| epoch 841 | iter 1 / 1 | time 0[s] | loss 0.82\n",
            "| epoch 842 | iter 1 / 1 | time 0[s] | loss 0.54\n",
            "| epoch 843 | iter 1 / 1 | time 0[s] | loss 0.82\n",
            "| epoch 844 | iter 1 / 1 | time 0[s] | loss 0.94\n",
            "| epoch 845 | iter 1 / 1 | time 0[s] | loss 0.69\n",
            "| epoch 846 | iter 1 / 1 | time 0[s] | loss 0.65\n",
            "| epoch 847 | iter 1 / 1 | time 0[s] | loss 0.82\n",
            "| epoch 848 | iter 1 / 1 | time 0[s] | loss 0.82\n",
            "| epoch 849 | iter 1 / 1 | time 0[s] | loss 0.94\n",
            "| epoch 850 | iter 1 / 1 | time 0[s] | loss 0.82\n",
            "| epoch 851 | iter 1 / 1 | time 0[s] | loss 0.65\n",
            "| epoch 852 | iter 1 / 1 | time 0[s] | loss 0.93\n",
            "| epoch 853 | iter 1 / 1 | time 0[s] | loss 0.65\n",
            "| epoch 854 | iter 1 / 1 | time 0[s] | loss 0.93\n",
            "| epoch 855 | iter 1 / 1 | time 0[s] | loss 0.53\n",
            "| epoch 856 | iter 1 / 1 | time 0[s] | loss 0.69\n",
            "| epoch 857 | iter 1 / 1 | time 0[s] | loss 0.81\n",
            "| epoch 858 | iter 1 / 1 | time 0[s] | loss 0.54\n",
            "| epoch 859 | iter 1 / 1 | time 0[s] | loss 0.81\n",
            "| epoch 860 | iter 1 / 1 | time 0[s] | loss 0.81\n",
            "| epoch 861 | iter 1 / 1 | time 0[s] | loss 0.54\n",
            "| epoch 862 | iter 1 / 1 | time 0[s] | loss 0.81\n",
            "| epoch 863 | iter 1 / 1 | time 0[s] | loss 0.93\n",
            "| epoch 864 | iter 1 / 1 | time 0[s] | loss 0.53\n",
            "| epoch 865 | iter 1 / 1 | time 0[s] | loss 0.69\n",
            "| epoch 866 | iter 1 / 1 | time 0[s] | loss 0.53\n",
            "| epoch 867 | iter 1 / 1 | time 0[s] | loss 0.81\n",
            "| epoch 868 | iter 1 / 1 | time 0[s] | loss 0.69\n",
            "| epoch 869 | iter 1 / 1 | time 0[s] | loss 0.80\n",
            "| epoch 870 | iter 1 / 1 | time 0[s] | loss 0.65\n",
            "| epoch 871 | iter 1 / 1 | time 0[s] | loss 0.81\n",
            "| epoch 872 | iter 1 / 1 | time 0[s] | loss 0.69\n",
            "| epoch 873 | iter 1 / 1 | time 0[s] | loss 0.52\n",
            "| epoch 874 | iter 1 / 1 | time 0[s] | loss 0.52\n",
            "| epoch 875 | iter 1 / 1 | time 0[s] | loss 0.80\n",
            "| epoch 876 | iter 1 / 1 | time 0[s] | loss 0.80\n",
            "| epoch 877 | iter 1 / 1 | time 0[s] | loss 0.65\n",
            "| epoch 878 | iter 1 / 1 | time 0[s] | loss 0.65\n",
            "| epoch 879 | iter 1 / 1 | time 0[s] | loss 0.80\n",
            "| epoch 880 | iter 1 / 1 | time 0[s] | loss 0.54\n",
            "| epoch 881 | iter 1 / 1 | time 0[s] | loss 0.69\n",
            "| epoch 882 | iter 1 / 1 | time 0[s] | loss 0.51\n",
            "| epoch 883 | iter 1 / 1 | time 0[s] | loss 0.79\n",
            "| epoch 884 | iter 1 / 1 | time 0[s] | loss 0.82\n",
            "| epoch 885 | iter 1 / 1 | time 0[s] | loss 0.69\n",
            "| epoch 886 | iter 1 / 1 | time 0[s] | loss 0.93\n",
            "| epoch 887 | iter 1 / 1 | time 0[s] | loss 0.69\n",
            "| epoch 888 | iter 1 / 1 | time 0[s] | loss 0.51\n",
            "| epoch 889 | iter 1 / 1 | time 0[s] | loss 0.54\n",
            "| epoch 890 | iter 1 / 1 | time 0[s] | loss 0.79\n",
            "| epoch 891 | iter 1 / 1 | time 0[s] | loss 0.64\n",
            "| epoch 892 | iter 1 / 1 | time 0[s] | loss 0.51\n",
            "| epoch 893 | iter 1 / 1 | time 0[s] | loss 0.65\n",
            "| epoch 894 | iter 1 / 1 | time 0[s] | loss 0.82\n",
            "| epoch 895 | iter 1 / 1 | time 0[s] | loss 0.68\n",
            "| epoch 896 | iter 1 / 1 | time 0[s] | loss 0.64\n",
            "| epoch 897 | iter 1 / 1 | time 0[s] | loss 0.64\n",
            "| epoch 898 | iter 1 / 1 | time 0[s] | loss 0.54\n",
            "| epoch 899 | iter 1 / 1 | time 0[s] | loss 0.50\n",
            "| epoch 900 | iter 1 / 1 | time 0[s] | loss 0.64\n",
            "| epoch 901 | iter 1 / 1 | time 0[s] | loss 0.83\n",
            "| epoch 902 | iter 1 / 1 | time 0[s] | loss 0.50\n",
            "| epoch 903 | iter 1 / 1 | time 0[s] | loss 0.93\n",
            "| epoch 904 | iter 1 / 1 | time 0[s] | loss 0.68\n",
            "| epoch 905 | iter 1 / 1 | time 0[s] | loss 0.53\n",
            "| epoch 906 | iter 1 / 1 | time 0[s] | loss 0.82\n",
            "| epoch 907 | iter 1 / 1 | time 0[s] | loss 0.79\n",
            "| epoch 908 | iter 1 / 1 | time 0[s] | loss 0.93\n",
            "| epoch 909 | iter 1 / 1 | time 0[s] | loss 0.82\n",
            "| epoch 910 | iter 1 / 1 | time 0[s] | loss 0.49\n",
            "| epoch 911 | iter 1 / 1 | time 0[s] | loss 0.64\n",
            "| epoch 912 | iter 1 / 1 | time 0[s] | loss 0.79\n",
            "| epoch 913 | iter 1 / 1 | time 0[s] | loss 0.63\n",
            "| epoch 914 | iter 1 / 1 | time 0[s] | loss 0.63\n",
            "| epoch 915 | iter 1 / 1 | time 0[s] | loss 0.79\n",
            "| epoch 916 | iter 1 / 1 | time 0[s] | loss 0.68\n",
            "| epoch 917 | iter 1 / 1 | time 0[s] | loss 0.64\n",
            "| epoch 918 | iter 1 / 1 | time 0[s] | loss 0.64\n",
            "| epoch 919 | iter 1 / 1 | time 0[s] | loss 0.63\n",
            "| epoch 920 | iter 1 / 1 | time 0[s] | loss 0.83\n",
            "| epoch 921 | iter 1 / 1 | time 0[s] | loss 0.82\n",
            "| epoch 922 | iter 1 / 1 | time 0[s] | loss 0.79\n",
            "| epoch 923 | iter 1 / 1 | time 0[s] | loss 0.64\n",
            "| epoch 924 | iter 1 / 1 | time 0[s] | loss 0.83\n",
            "| epoch 925 | iter 1 / 1 | time 0[s] | loss 0.63\n",
            "| epoch 926 | iter 1 / 1 | time 0[s] | loss 0.52\n",
            "| epoch 927 | iter 1 / 1 | time 0[s] | loss 0.79\n",
            "| epoch 928 | iter 1 / 1 | time 0[s] | loss 0.63\n",
            "| epoch 929 | iter 1 / 1 | time 0[s] | loss 0.52\n",
            "| epoch 930 | iter 1 / 1 | time 0[s] | loss 0.79\n",
            "| epoch 931 | iter 1 / 1 | time 0[s] | loss 0.82\n",
            "| epoch 932 | iter 1 / 1 | time 0[s] | loss 0.52\n",
            "| epoch 933 | iter 1 / 1 | time 0[s] | loss 0.82\n",
            "| epoch 934 | iter 1 / 1 | time 0[s] | loss 0.63\n",
            "| epoch 935 | iter 1 / 1 | time 0[s] | loss 0.63\n",
            "| epoch 936 | iter 1 / 1 | time 0[s] | loss 0.49\n",
            "| epoch 937 | iter 1 / 1 | time 0[s] | loss 0.80\n",
            "| epoch 938 | iter 1 / 1 | time 0[s] | loss 0.79\n",
            "| epoch 939 | iter 1 / 1 | time 0[s] | loss 0.82\n",
            "| epoch 940 | iter 1 / 1 | time 0[s] | loss 0.80\n",
            "| epoch 941 | iter 1 / 1 | time 0[s] | loss 0.51\n",
            "| epoch 942 | iter 1 / 1 | time 0[s] | loss 0.80\n",
            "| epoch 943 | iter 1 / 1 | time 0[s] | loss 0.51\n",
            "| epoch 944 | iter 1 / 1 | time 0[s] | loss 0.51\n",
            "| epoch 945 | iter 1 / 1 | time 0[s] | loss 0.68\n",
            "| epoch 946 | iter 1 / 1 | time 0[s] | loss 0.63\n",
            "| epoch 947 | iter 1 / 1 | time 0[s] | loss 0.80\n",
            "| epoch 948 | iter 1 / 1 | time 0[s] | loss 0.49\n",
            "| epoch 949 | iter 1 / 1 | time 0[s] | loss 0.82\n",
            "| epoch 950 | iter 1 / 1 | time 0[s] | loss 0.81\n",
            "| epoch 951 | iter 1 / 1 | time 0[s] | loss 0.67\n",
            "| epoch 952 | iter 1 / 1 | time 0[s] | loss 0.79\n",
            "| epoch 953 | iter 1 / 1 | time 0[s] | loss 0.93\n",
            "| epoch 954 | iter 1 / 1 | time 0[s] | loss 0.67\n",
            "| epoch 955 | iter 1 / 1 | time 0[s] | loss 0.80\n",
            "| epoch 956 | iter 1 / 1 | time 0[s] | loss 0.79\n",
            "| epoch 957 | iter 1 / 1 | time 0[s] | loss 0.62\n",
            "| epoch 958 | iter 1 / 1 | time 0[s] | loss 0.79\n",
            "| epoch 959 | iter 1 / 1 | time 0[s] | loss 0.49\n",
            "| epoch 960 | iter 1 / 1 | time 0[s] | loss 0.81\n",
            "| epoch 961 | iter 1 / 1 | time 0[s] | loss 0.62\n",
            "| epoch 962 | iter 1 / 1 | time 0[s] | loss 0.63\n",
            "| epoch 963 | iter 1 / 1 | time 0[s] | loss 0.81\n",
            "| epoch 964 | iter 1 / 1 | time 0[s] | loss 0.48\n",
            "| epoch 965 | iter 1 / 1 | time 0[s] | loss 0.62\n",
            "| epoch 966 | iter 1 / 1 | time 0[s] | loss 0.62\n",
            "| epoch 967 | iter 1 / 1 | time 0[s] | loss 0.67\n",
            "| epoch 968 | iter 1 / 1 | time 0[s] | loss 0.48\n",
            "| epoch 969 | iter 1 / 1 | time 0[s] | loss 0.81\n",
            "| epoch 970 | iter 1 / 1 | time 0[s] | loss 0.93\n",
            "| epoch 971 | iter 1 / 1 | time 0[s] | loss 0.82\n",
            "| epoch 972 | iter 1 / 1 | time 0[s] | loss 0.81\n",
            "| epoch 973 | iter 1 / 1 | time 0[s] | loss 0.93\n",
            "| epoch 974 | iter 1 / 1 | time 0[s] | loss 0.78\n",
            "| epoch 975 | iter 1 / 1 | time 0[s] | loss 0.67\n",
            "| epoch 976 | iter 1 / 1 | time 0[s] | loss 0.48\n",
            "| epoch 977 | iter 1 / 1 | time 0[s] | loss 0.93\n",
            "| epoch 978 | iter 1 / 1 | time 0[s] | loss 0.79\n",
            "| epoch 979 | iter 1 / 1 | time 0[s] | loss 0.81\n",
            "| epoch 980 | iter 1 / 1 | time 0[s] | loss 0.51\n",
            "| epoch 981 | iter 1 / 1 | time 0[s] | loss 0.48\n",
            "| epoch 982 | iter 1 / 1 | time 0[s] | loss 0.92\n",
            "| epoch 983 | iter 1 / 1 | time 0[s] | loss 0.78\n",
            "| epoch 984 | iter 1 / 1 | time 0[s] | loss 0.92\n",
            "| epoch 985 | iter 1 / 1 | time 0[s] | loss 0.48\n",
            "| epoch 986 | iter 1 / 1 | time 0[s] | loss 0.78\n",
            "| epoch 987 | iter 1 / 1 | time 0[s] | loss 0.51\n",
            "| epoch 988 | iter 1 / 1 | time 0[s] | loss 0.63\n",
            "| epoch 989 | iter 1 / 1 | time 0[s] | loss 0.92\n",
            "| epoch 990 | iter 1 / 1 | time 0[s] | loss 0.62\n",
            "| epoch 991 | iter 1 / 1 | time 0[s] | loss 0.48\n",
            "| epoch 992 | iter 1 / 1 | time 0[s] | loss 0.67\n",
            "| epoch 993 | iter 1 / 1 | time 0[s] | loss 0.63\n",
            "| epoch 994 | iter 1 / 1 | time 0[s] | loss 0.92\n",
            "| epoch 995 | iter 1 / 1 | time 0[s] | loss 0.48\n",
            "| epoch 996 | iter 1 / 1 | time 0[s] | loss 0.62\n",
            "| epoch 997 | iter 1 / 1 | time 0[s] | loss 0.81\n",
            "| epoch 998 | iter 1 / 1 | time 0[s] | loss 0.92\n",
            "| epoch 999 | iter 1 / 1 | time 0[s] | loss 0.81\n",
            "| epoch 1000 | iter 1 / 1 | time 0[s] | loss 0.67\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAe8AAAFYCAYAAAB6RnQAAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "pXQCFOgqscdm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "outputId": "8d0ead31-b3e2-4744-f90f-37b320558ca1"
      },
      "cell_type": "code",
      "source": [
        "word_vecs = model.word_vecs\n",
        "for word_id, word in id_to_word.items():\n",
        "  print(word, word_vecs[word_id])"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "you [ 0.79927504 -0.941188    0.82642955  0.76557076 -0.84946984]\n",
            "say [-0.8203432  -0.9735661  -0.81836295 -0.83937687  0.8166195 ]\n",
            "goodbye [ 0.7416605  -0.6130347   0.73302007  0.7558204  -0.65633756]\n",
            "and [-0.8120822  -0.9872426  -0.823234   -0.8404246   0.81870663]\n",
            "i [ 0.7181531  -0.564209    0.71903753  0.72238237 -0.6436246 ]\n",
            "hello [ 0.8155345  -0.9283557   0.81489     0.76903975 -0.8401285 ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ECb7vVYwuyAr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "目的: 単語の分散表現を得たい\n",
        "\n",
        "手法:\n",
        "1. カウントベースの手法\n",
        "2. 推論ベースの手法(word2vec)"
      ]
    },
    {
      "metadata": {
        "id": "h06QYm6yuctr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "1. カウントベースの手法では、周囲の単語の頻度によって、共起行列を作り、その行列に対して、SVDを適応することで、密なベクトル単語の分散表現を獲得する。"
      ]
    },
    {
      "metadata": {
        "id": "SL6Hc1Agvf2Z",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "2. 推論ベースの手法では、推論することを目標として、その副産物として、単語の分散表現を得る"
      ]
    },
    {
      "metadata": {
        "id": "0f4z9vfww0wD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "単語の類似性に関する定量評価に関して言えば、推論ベースとカウントベースの手法には優劣がつけられない\n",
        "\n",
        "word2vecでは、類似性に加えて、複雑な単語間のパターンも捉えることができる。\n",
        "\n",
        "推論ベースの手法では、パラメータの再学習を行うことができる。"
      ]
    },
    {
      "metadata": {
        "id": "7H8xWP6GvHFl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "推論ベースの手法(word2vec) : \n",
        "\n",
        "i. skip-gramモデル\n",
        "\n",
        "ii. CBOWモデル\n",
        "\n",
        "\n",
        "\n",
        "i. skip-gramでは、一つの単語(ターゲット)から、複数の単語(コンテキスト)を推測する。\n",
        "\n",
        "\n",
        "\n",
        "ii. CBOWでは、複数の単語(コンテキスト)から一つの単語(ターゲット)を推測する"
      ]
    },
    {
      "metadata": {
        "id": "Cdx4qlYzwICo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**word2vecは重みの再学習を行えるので、単語の分散表現の更新や、追加を効率的に行える**"
      ]
    }
  ]
}